{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973be361",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-and-Background\" data-toc-modified-id=\"Introduction-and-Background-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction and Background</a></span><ul class=\"toc-item\"><li><span><a href=\"#Purpose-of-Logistic-Regression\" data-toc-modified-id=\"Purpose-of-Logistic-Regression-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Purpose of Logistic Regression</a></span></li><li><span><a href=\"#Distinction-from-Linear-Regression\" data-toc-modified-id=\"Distinction-from-Linear-Regression-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Distinction from Linear Regression</a></span></li><li><span><a href=\"#Applications-of-Logistic-Regression\" data-toc-modified-id=\"Applications-of-Logistic-Regression-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Applications of Logistic Regression</a></span></li><li><span><a href=\"#Binary-and-Multinomial-Logistic-Regression\" data-toc-modified-id=\"Binary-and-Multinomial-Logistic-Regression-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Binary and Multinomial Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Binary-Logistic-Regression\" data-toc-modified-id=\"Binary-Logistic-Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Binary Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Purpose-of-Binary-Logistic-Regression\" data-toc-modified-id=\"Purpose-of-Binary-Logistic-Regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Purpose of Binary Logistic Regression</a></span></li><li><span><a href=\"#Sigmoid-Function\" data-toc-modified-id=\"Sigmoid-Function-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Sigmoid Function</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Decision-Boundary\" data-toc-modified-id=\"Decision-Boundary-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Decision Boundary</a></span></li><li><span><a href=\"#Thresholding\" data-toc-modified-id=\"Thresholding-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Thresholding</a></span></li><li><span><a href=\"#Use-Cases-of-Binary-Logistic-Regression\" data-toc-modified-id=\"Use-Cases-of-Binary-Logistic-Regression-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Use Cases of Binary Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Multinomial-Logistic-Regression\" data-toc-modified-id=\"Multinomial-Logistic-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Multinomial Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Purpose-of-Multinomial-Logistic-Regression\" data-toc-modified-id=\"Purpose-of-Multinomial-Logistic-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Purpose of Multinomial Logistic Regression</a></span></li><li><span><a href=\"#The-Softmax-Function\" data-toc-modified-id=\"The-Softmax-Function-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The Softmax Function</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Decision-Boundary\" data-toc-modified-id=\"Decision-Boundary-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Decision Boundary</a></span></li><li><span><a href=\"#Use-Cases-of-Multinomial-Logistic-Regression\" data-toc-modified-id=\"Use-Cases-of-Multinomial-Logistic-Regression-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Use Cases of Multinomial Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Binary-Logistic-Regression-Formula\" data-toc-modified-id=\"Binary-Logistic-Regression-Formula-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Binary Logistic Regression Formula</a></span><ul class=\"toc-item\"><li><span><a href=\"#Log-Odds\" data-toc-modified-id=\"Log-Odds-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Log-Odds</a></span></li><li><span><a href=\"#Sigmoid-Function\" data-toc-modified-id=\"Sigmoid-Function-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Sigmoid Function</a></span></li></ul></li><li><span><a href=\"#Multinomial-Logistic-Regression-Formula\" data-toc-modified-id=\"Multinomial-Logistic-Regression-Formula-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Multinomial Logistic Regression Formula</a></span><ul class=\"toc-item\"><li><span><a href=\"#Softmax-Function\" data-toc-modified-id=\"Softmax-Function-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Softmax Function</a></span></li><li><span><a href=\"#Model-Training\" data-toc-modified-id=\"Model-Training-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Model Training</a></span></li><li><span><a href=\"#Decision-Boundary\" data-toc-modified-id=\"Decision-Boundary-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Decision Boundary</a></span></li><li><span><a href=\"#Probability-Assignment\" data-toc-modified-id=\"Probability-Assignment-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Probability Assignment</a></span></li></ul></li><li><span><a href=\"#Code-Examples-for-Binary-Logistic-Regression\" data-toc-modified-id=\"Code-Examples-for-Binary-Logistic-Regression-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Code Examples for Binary Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Model-Fitting\" data-toc-modified-id=\"Model-Fitting-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Model Fitting</a></span></li><li><span><a href=\"#Interpretation-of-Results\" data-toc-modified-id=\"Interpretation-of-Results-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Interpretation of Results</a></span></li></ul></li><li><span><a href=\"#Code-Examples-for-Multinomial-Logistic-Regression\" data-toc-modified-id=\"Code-Examples-for-Multinomial-Logistic-Regression-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Code Examples for Multinomial Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Loading\" data-toc-modified-id=\"Data-Loading-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Data Loading</a></span></li><li><span><a href=\"#Model-Fitting\" data-toc-modified-id=\"Model-Fitting-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Model Fitting</a></span></li><li><span><a href=\"#Interpretation-of-Results\" data-toc-modified-id=\"Interpretation-of-Results-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Interpretation of Results</a></span></li></ul></li><li><span><a href=\"#Model-Evaluation-for-Logistic-Regression\" data-toc-modified-id=\"Model-Evaluation-for-Logistic-Regression-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Model Evaluation for Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Common-Evaluation-Metrics\" data-toc-modified-id=\"Common-Evaluation-Metrics-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Common Evaluation Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#Precision\" data-toc-modified-id=\"Precision-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;</span>Precision</a></span></li><li><span><a href=\"#Recall-(Sensitivity)\" data-toc-modified-id=\"Recall-(Sensitivity)-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;</span>Recall (Sensitivity)</a></span></li><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-8.1.4\"><span class=\"toc-item-num\">8.1.4&nbsp;&nbsp;</span>F1-Score</a></span></li></ul></li><li><span><a href=\"#Code-Examples-for-Model-Evaluation\" data-toc-modified-id=\"Code-Examples-for-Model-Evaluation-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Code Examples for Model Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-Logistic-Regression-Evaluation\" data-toc-modified-id=\"Binary-Logistic-Regression-Evaluation-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;</span>Binary Logistic Regression Evaluation</a></span></li><li><span><a href=\"#Multinomial-Logistic-Regression-Evaluation\" data-toc-modified-id=\"Multinomial-Logistic-Regression-Evaluation-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;</span>Multinomial Logistic Regression Evaluation</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Engineering-for-Logistic-Regression\" data-toc-modified-id=\"Feature-Engineering-for-Logistic-Regression-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Feature Engineering for Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Encoding-Categorical-Variables\" data-toc-modified-id=\"Encoding-Categorical-Variables-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Encoding Categorical Variables</a></span></li><li><span><a href=\"#Handling-Missing-Data\" data-toc-modified-id=\"Handling-Missing-Data-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Handling Missing Data</a></span></li></ul></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importance-of-Hyperparameter-Tuning\" data-toc-modified-id=\"Importance-of-Hyperparameter-Tuning-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Importance of Hyperparameter Tuning</a></span></li><li><span><a href=\"#Grid-Search-and-Random-Search\" data-toc-modified-id=\"Grid-Search-and-Random-Search-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;</span>Grid Search and Random Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-10.2.1\"><span class=\"toc-item-num\">10.2.1&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Random-Search\" data-toc-modified-id=\"Random-Search-10.2.2\"><span class=\"toc-item-num\">10.2.2&nbsp;&nbsp;</span>Random Search</a></span></li></ul></li><li><span><a href=\"#Code-Examples-for-Hyperparameter-Tuning\" data-toc-modified-id=\"Code-Examples-for-Hyperparameter-Tuning-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;</span>Code Examples for Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search-for-Binary-Logistic-Regression\" data-toc-modified-id=\"Grid-Search-for-Binary-Logistic-Regression-10.3.1\"><span class=\"toc-item-num\">10.3.1&nbsp;&nbsp;</span>Grid Search for Binary Logistic Regression</a></span></li><li><span><a href=\"#Random-Search-for-Multinomial-Logistic-Regression\" data-toc-modified-id=\"Random-Search-for-Multinomial-Logistic-Regression-10.3.2\"><span class=\"toc-item-num\">10.3.2&nbsp;&nbsp;</span>Random Search for Multinomial Logistic Regression</a></span></li></ul></li></ul></li><li><span><a href=\"#Real-Life-Use-Cases\" data-toc-modified-id=\"Real-Life-Use-Cases-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Real-Life Use Cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Binary-Logistic-Regression-Use-Cases\" data-toc-modified-id=\"Binary-Logistic-Regression-Use-Cases-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;</span>Binary Logistic Regression Use Cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Medical-Diagnosis\" data-toc-modified-id=\"Medical-Diagnosis-11.1.1\"><span class=\"toc-item-num\">11.1.1&nbsp;&nbsp;</span>Medical Diagnosis</a></span></li><li><span><a href=\"#Credit-Risk-Assessment\" data-toc-modified-id=\"Credit-Risk-Assessment-11.1.2\"><span class=\"toc-item-num\">11.1.2&nbsp;&nbsp;</span>Credit Risk Assessment</a></span></li><li><span><a href=\"#Customer-Churn-Prediction\" data-toc-modified-id=\"Customer-Churn-Prediction-11.1.3\"><span class=\"toc-item-num\">11.1.3&nbsp;&nbsp;</span>Customer Churn Prediction</a></span></li></ul></li><li><span><a href=\"#Multinomial-Logistic-Regression-Use-Cases\" data-toc-modified-id=\"Multinomial-Logistic-Regression-Use-Cases-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;</span>Multinomial Logistic Regression Use Cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-Processing-(NLP)\" data-toc-modified-id=\"Natural-Language-Processing-(NLP)-11.2.1\"><span class=\"toc-item-num\">11.2.1&nbsp;&nbsp;</span>Natural Language Processing (NLP)</a></span></li><li><span><a href=\"#Image-Classification\" data-toc-modified-id=\"Image-Classification-11.2.2\"><span class=\"toc-item-num\">11.2.2&nbsp;&nbsp;</span>Image Classification</a></span></li><li><span><a href=\"#Species-Classification\" data-toc-modified-id=\"Species-Classification-11.2.3\"><span class=\"toc-item-num\">11.2.3&nbsp;&nbsp;</span>Species Classification</a></span></li></ul></li></ul></li><li><span><a href=\"#Content-Summarization\" data-toc-modified-id=\"Content-Summarization-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Content Summarization</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb3904",
   "metadata": {},
   "source": [
    "# Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6233d1",
   "metadata": {},
   "source": [
    "In the realm of machine learning and statistical modeling, Logistic Regression stands as a fundamental technique that plays a pivotal role in solving classification problems. Unlike linear regression, which is primarily used for predicting continuous numerical values, Logistic Regression finds its purpose in predicting categorical outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb23dc6",
   "metadata": {},
   "source": [
    "## Purpose of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fcafd",
   "metadata": {},
   "source": [
    "Logistic Regression serves the primary objective of estimating the probability of a binary outcome. It is particularly well-suited for addressing questions like:\n",
    "- Will a customer purchase a product? (yes/no)\n",
    "- Will a patient develop a disease? (positive/negative)\n",
    "- Will an email be classified as spam? (spam/ham)\n",
    "\n",
    "The beauty of Logistic Regression is its ability to quantify these probabilities, making it a powerful tool in scenarios where making binary decisions is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d21cd5",
   "metadata": {},
   "source": [
    "## Distinction from Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cf1d9",
   "metadata": {},
   "source": [
    "At first glance, the name \"Logistic Regression\" might imply that it's a regression technique similar to Linear Regression. However, it's important to note the critical distinction:\n",
    "\n",
    "- Linear Regression: Predicts a continuous numeric value (e.g., house price, temperature).\n",
    "- Logistic Regression: Predicts the probability of a binary categorical outcome (e.g., yes/no, 1/0).\n",
    "\n",
    "Logistic Regression models the relationship between the independent variables (features) and the log-odds of a binary response variable. It does so using the logistic function, also known as the sigmoid function, to constrain the output within the range of [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038095d",
   "metadata": {},
   "source": [
    "## Applications of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231400a6",
   "metadata": {},
   "source": [
    "Logistic Regression finds applications in various fields, including but not limited to:\n",
    "\n",
    "- **Medicine:** Predicting whether a patient has a particular disease based on medical test results.\n",
    "- **Marketing:** Determining the likelihood of a customer making a purchase.\n",
    "- **Finance:** Identifying fraudulent transactions or credit defaulters.\n",
    "- **Natural Language Processing (NLP):** Text classification, such as spam detection.\n",
    "- **Social Sciences:** Analyzing factors affecting voting behavior or survey responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c3bac5",
   "metadata": {},
   "source": [
    "## Binary and Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed503e",
   "metadata": {},
   "source": [
    "- **Binary Logistic Regression:** Designed for problems with two possible outcomes (e.g., yes/no, true/false).\n",
    "- **Multinomial Logistic Regression:** Tailored for multi-class classification problems with more than two possible outcomes (e.g., categories, classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6051dcd",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484e61e",
   "metadata": {},
   "source": [
    "Binary Logistic Regression is a widely used statistical method for tackling classification tasks with two possible outcomes. These two outcomes are typically represented as:\n",
    "- **Positive Class (1):** This class signifies the presence or occurrence of an event or condition.\n",
    "- **Negative Class (0):** This class denotes the absence or non-occurrence of the event or condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb9d0fb",
   "metadata": {},
   "source": [
    "## Purpose of Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2dbd2",
   "metadata": {},
   "source": [
    "The primary goal of Binary Logistic Regression is to model and estimate the probability of an observation belonging to the positive class (1). This modeling is achieved by creating a decision boundary, often represented as a sigmoid function, which transforms the linear combination of predictor variables into probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0b05b",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f7c23",
   "metadata": {},
   "source": [
    "The core of Binary Logistic Regression is the sigmoid function, also known as the logistic function. This function maps any real-valued number to a value within the range [0, 1]. The sigmoid function is defined as:\n",
    "\n",
    "$$ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n)}} $$\n",
    "\n",
    "Where:\n",
    "- $P(Y = 1|X)$ represents the probability of the positive class (1) given the predictor variables $X$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients or weights assigned to the predictor variables $X_1, X_2, \\ldots, X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2fe4c5",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ef9b4",
   "metadata": {},
   "source": [
    "To fit a Binary Logistic Regression model, the goal is to find the optimal set of coefficients $\\beta$ that maximize the likelihood of the observed outcomes. This process often involves iterative optimization techniques such as maximum likelihood estimation (MLE) or gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ed656",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326a7b1",
   "metadata": {},
   "source": [
    "The decision boundary separates the feature space into regions where the probability of the positive class is higher or lower. In a two-dimensional feature space, the decision boundary is a curve. In higher dimensions, it's a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b8a32",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5df6b8",
   "metadata": {},
   "source": [
    "While Binary Logistic Regression provides probability estimates, predictions are typically made by applying a threshold to these probabilities. Commonly, a threshold of 0.5 is used: if $P(Y = 1|X) \\geq 0.5$, the observation is classified as the positive class (1); otherwise, it's classified as the negative class (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e91ea8",
   "metadata": {},
   "source": [
    "## Use Cases of Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3054a12",
   "metadata": {},
   "source": [
    "Binary Logistic Regression finds application in various domains, including:\n",
    "- **Medical Diagnostics:** Predicting whether a patient has a specific disease based on medical tests.\n",
    "- **Credit Risk Assessment:** Determining whether a loan applicant is likely to default.\n",
    "- **Marketing:** Identifying potential customers who are likely to make a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecb4324",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a89694",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression is an extension of the binary logistic regression to handle multi-class classification tasks, where there are more than two possible outcomes or categories. This method is particularly useful when you need to classify data into multiple non-ordered classes or categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498bfc1",
   "metadata": {},
   "source": [
    "## Purpose of Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750dd62",
   "metadata": {},
   "source": [
    "The primary purpose of Multinomial Logistic Regression is to predict the probability of an observation belonging to one of several mutually exclusive classes. These classes are typically nominal, meaning there's no inherent order or ranking among them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18407f",
   "metadata": {},
   "source": [
    "## The Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12110b5d",
   "metadata": {},
   "source": [
    "In Multinomial Logistic Regression, we use the softmax function, also known as the normalized exponential, to assign probabilities to each class. The softmax function generalizes the sigmoid function used in binary logistic regression. It transforms a vector of real numbers into a probability distribution over multiple classes. The softmax function is defined as follows:\n",
    "\n",
    "$$ P(Y = k|X) = \\frac{e^{\\beta_k \\cdot X}}{\\sum_{j=1}^{K} e^{\\beta_j \\cdot X}} $$\n",
    "\n",
    "Where:\n",
    "- $P(Y = k|X)$ represents the probability of the observation belonging to class $k$ given the predictor variables $X$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_k$ is the set of coefficients associated with class $k$.\n",
    "- $K$ is the total number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bc913",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647c3bb",
   "metadata": {},
   "source": [
    "The training process of Multinomial Logistic Regression is similar to that of Binary Logistic Regression. The objective is to find the optimal set of coefficients $\\beta$ for each class that maximizes the likelihood of the observed outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4551f0",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0705ec",
   "metadata": {},
   "source": [
    "Unlike binary classification, Multinomial Logistic Regression does not have a single decision boundary. Instead, it assigns probabilities to each class, and the class with the highest probability is the predicted outcome. The decision boundaries in multi-class problems are complex, and there can be more than one class with a high probability for a given observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f4c61",
   "metadata": {},
   "source": [
    "## Use Cases of Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46436c2",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression is applied in numerous scenarios, including:\n",
    "- **Natural Language Processing (NLP):** Text classification into multiple categories.\n",
    "- **Image Classification:** Assigning images to multiple classes.\n",
    "- **Healthcare:** Diagnosing diseases with multiple possible outcomes.\n",
    "- **Customer Feedback:** Categorizing customer reviews into different sentiment classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119426bc",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d70ef",
   "metadata": {},
   "source": [
    "Binary Logistic Regression relies on a mathematical formula to estimate the probability of an observation belonging to the positive class (1). The core of this formula is the log-odds, which are then transformed using the sigmoid function to constrain the output within the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a22c49c",
   "metadata": {},
   "source": [
    "## Log-Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea90249",
   "metadata": {},
   "source": [
    "The log-odds, often referred to as the logit, represent the natural logarithm of the odds of the positive class (1) occurring. The formula for log-odds is given as:\n",
    "\n",
    "$$ \\text{Log-Odds} = \\ln\\left(\\frac{P(Y = 1|X)}{1 - P(Y = 1|X)}\\right) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n $$\n",
    "\n",
    "Where:\n",
    "- $P(Y = 1|X)$ represents the probability of the positive class (1) given the predictor variables $X$.\n",
    "- $\\ln$ denotes the natural logarithm.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients or weights assigned to the predictor variables $X_1, X_2, \\ldots, X_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58473505",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2c381",
   "metadata": {},
   "source": [
    "The log-odds are not very intuitive for interpretation, and they can take any real value. To convert these log-odds into probabilities that fall within the range [0, 1], Binary Logistic Regression employs the sigmoid function, also known as the logistic function. The sigmoid function is defined as:\n",
    "\n",
    "$$ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n)}} $$\n",
    "\n",
    "Where:\n",
    "- $P(Y = 1|X)$ represents the probability of the positive class (1) given the predictor variables $X$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n$ are the coefficients or weights assigned to the predictor variables $X_1, X_2, \\ldots, X_n$.\n",
    "\n",
    "The sigmoid function transforms the log-odds into a probability value that is interpretable as the likelihood of an observation belonging to the positive class.\n",
    "\n",
    "In the context of Binary Logistic Regression, this formula serves as the foundation for modeling the relationship between predictor variables and the probability of the positive class. The coefficients $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n$ are estimated through the model training process to make accurate probability predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22ca4d",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32838e65",
   "metadata": {},
   "source": [
    "Multinomial Logistic Regression extends the principles of binary logistic regression to handle multi-class classification problems. It employs the softmax function to estimate the probability of an observation belonging to one of several mutually exclusive classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591558a",
   "metadata": {},
   "source": [
    "## Softmax Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cf360",
   "metadata": {},
   "source": [
    "The central component of Multinomial Logistic Regression is the softmax function. The softmax function transforms a vector of real numbers into a probability distribution over multiple classes. It is particularly useful for assigning probabilities to each class. The formula for the softmax function is given as:\n",
    "\n",
    "$$ P(Y = k|X) = \\frac{e^{\\beta_k \\cdot X}}{\\sum_{j=1}^{K} e^{\\beta_j \\cdot X}} $$\n",
    "\n",
    "Where:\n",
    "- $P(Y = k|X)$ represents the probability of the observation belonging to class $k$ given the predictor variables $X$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "- $\\beta_k$ is the set of coefficients associated with class $k$.\n",
    "- $K$ is the total number of classes.\n",
    "\n",
    "In this formula, the softmax function normalizes the exponentiated linear combination of the predictor variables by the sum of exponentials over all classes. This ensures that the probabilities sum to 1, creating a valid probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaca894",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f444edd",
   "metadata": {},
   "source": [
    "The training process of Multinomial Logistic Regression aims to find the optimal set of coefficients $\\beta$ for each class that maximizes the likelihood of the observed outcomes. This process is often achieved using techniques such as maximum likelihood estimation (MLE) or gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c75c1e9",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3efe05",
   "metadata": {},
   "source": [
    "In Multinomial Logistic Regression, there isn't a single decision boundary as in binary classification. Instead, the model assigns probabilities to each class, and the class with the highest probability is considered the predicted outcome. The decision boundaries are complex and vary based on the number of classes and the distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef1b3c",
   "metadata": {},
   "source": [
    "## Probability Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5103f7b6",
   "metadata": {},
   "source": [
    "The probability of an observation belonging to each class is determined by the softmax function. The class with the highest assigned probability is the predicted class for that observation.\n",
    "\n",
    "Multinomial Logistic Regression is a versatile method for handling multi-class classification tasks. It's used when you need to categorize data into multiple non-ordered classes, making it invaluable in fields like natural language processing, image classification, healthcare, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70f82c",
   "metadata": {},
   "source": [
    "# Code Examples for Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af92ab",
   "metadata": {},
   "source": [
    "In this section, we will provide Python code examples to demonstrate how to implement Binary Logistic Regression using the popular library, scikit-learn. These examples will cover data loading, model fitting, and interpretation of results for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43296ab",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9f0a7",
   "metadata": {},
   "source": [
    "First, let's load a dataset that is suitable for binary classification. We will use the Breast Cancer Wisconsin dataset, which is available in scikit-learn and is commonly used for cancer diagnosis.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data  # Predictor variables\n",
    "y = data.target  # Target variable (binary: 0 for malignant, 1 for benign)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ef0c8",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60be6ac0",
   "metadata": {},
   "source": [
    "Now, let's fit a Binary Logistic Regression model to the data using scikit-learn. We will use the `LogisticRegression` class for this purpose.\n",
    "\n",
    "```python\n",
    "# Import the LogisticRegression class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Binary Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2e253",
   "metadata": {},
   "source": [
    "## Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35e1dd",
   "metadata": {},
   "source": [
    "After fitting the model, we can interpret the results by examining the coefficients and making predictions. The coefficients represent the weights assigned to each predictor variable.\n",
    "\n",
    "```python\n",
    "# Get the coefficients and intercept\n",
    "coefficients = logistic_model.coef_\n",
    "intercept = logistic_model.intercept_\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate model accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef9cf89",
   "metadata": {},
   "source": [
    "In the code above, we load the Breast Cancer Wisconsin dataset, create a Binary Logistic Regression model, fit it to the training data, and then make predictions on the test data. We also calculate the accuracy of the model to evaluate its performance.\n",
    "\n",
    "These code examples illustrate the practical implementation of Binary Logistic Regression for binary classification tasks using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ae0dc",
   "metadata": {},
   "source": [
    "# Code Examples for Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad7ca4",
   "metadata": {},
   "source": [
    "In this section, we will provide Python code examples to demonstrate how to implement Multinomial Logistic Regression using the popular library, scikit-learn. These examples will cover data loading, model fitting, and interpretation of results for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096dc3f",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64057850",
   "metadata": {},
   "source": [
    "First, let's load a dataset suitable for multi-class classification. We will use the famous Iris dataset, which contains three different species of iris flowers.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Predictor variables\n",
    "y = data.target  # Target variable (multi-class)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddb880",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db932b1",
   "metadata": {},
   "source": [
    "Now, let's fit a Multinomial Logistic Regression model to the data using scikit-learn. We will use the `LogisticRegression` class with the `multi_class` parameter set to \"multinomial.\"\n",
    "\n",
    "```python\n",
    "# Import the LogisticRegression class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Multinomial Logistic Regression model\n",
    "multinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# Fit the model to the training data\n",
    "multinomial_model.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8227d",
   "metadata": {},
   "source": [
    "## Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c1ac6",
   "metadata": {},
   "source": [
    "After fitting the model, we can interpret the results by examining the coefficients and making predictions.\n",
    "\n",
    "```python\n",
    "# Get the coefficients and intercept\n",
    "coefficients = multinomial_model.coef_\n",
    "intercept = multinomial_model.intercept_\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Intercept:\", intercept)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = multinomial_model.predict(X_test)\n",
    "\n",
    "# Calculate model accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c35d3b",
   "metadata": {},
   "source": [
    "In the code above, we load the Iris dataset, create a Multinomial Logistic Regression model, fit it to the training data, and then make predictions on the test data. We also calculate the accuracy of the model to evaluate its performance.\n",
    "\n",
    "These code examples illustrate the practical implementation of Multinomial Logistic Regression for multi-class classification tasks using scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beea844c",
   "metadata": {},
   "source": [
    "# Model Evaluation for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d834ad",
   "metadata": {},
   "source": [
    "Evaluating the performance of logistic regression models is crucial to determine how well they are classifying data. There are several metrics used for this purpose, and the choice of metric depends on the specific problem and the balance between different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107d6d9",
   "metadata": {},
   "source": [
    "## Common Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b08f1c",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9960f",
   "metadata": {},
   "source": [
    "Accuracy is a basic measure of overall model performance and represents the proportion of correctly classified instances over the total instances.\n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5148ed",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dc8c1",
   "metadata": {},
   "source": [
    "Precision quantifies the ability of the model to correctly identify positive instances out of all instances it predicted as positive. It's a crucial metric in situations where false positives are costly.\n",
    "\n",
    "$$ \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb1e06a",
   "metadata": {},
   "source": [
    "### Recall (Sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aaab7f",
   "metadata": {},
   "source": [
    "Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances out of all actual positive instances. It is vital when missing positive cases is costly.\n",
    "\n",
    "$$ \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f0b04",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64253e75",
   "metadata": {},
   "source": [
    "The F1-score is the harmonic mean of precision and recall. It balances the trade-off between precision and recall. A high F1-score indicates a model that performs well in both precision and recall.\n",
    "\n",
    "$$ \\text{F1-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffd012c",
   "metadata": {},
   "source": [
    "## Code Examples for Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8995546",
   "metadata": {},
   "source": [
    "Let's now see how to evaluate logistic regression models using code examples for both binary and multinomial logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92393c50",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01009b4e",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have predictions in y_pred and true labels in y_test\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Binary Logistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d5cb7",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65c531",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming you have predictions in y_pred_multinomial and true labels in y_test_multinomial\n",
    "accuracy_multinomial = accuracy_score(y_test_multinomial, y_pred_multinomial)\n",
    "precision_multinomial = precision_score(y_test_multinomial, y_pred_multinomial, average='weighted')\n",
    "recall_multinomial = recall_score(y_test_multinomial, y_pred_multinomial, average='weighted')\n",
    "f1_multinomial = f1_score(y_test_multinomial, y_pred_multinomial, average='weighted')\n",
    "\n",
    "print(\"Multinomial Logistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_multinomial)\n",
    "print(\"Precision (weighted):\", precision_multinomial)\n",
    "print(\"Recall (weighted):\", recall_multinomial)\n",
    "print(\"F1-Score (weighted):\", f1_multinomial)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20bbbc",
   "metadata": {},
   "source": [
    "These code examples demonstrate how to calculate and interpret common evaluation metrics for both binary and multinomial logistic regression models. Remember that the choice of metrics should be based on the specific problem and the importance of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637443ab",
   "metadata": {},
   "source": [
    "# Feature Engineering for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e7438",
   "metadata": {},
   "source": [
    "Effective feature engineering is crucial for the success of logistic regression models. In this section, we will explore the importance of feature selection, encoding categorical variables, and handling missing data in the context of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6e053",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164902b5",
   "metadata": {},
   "source": [
    "Feature selection involves choosing the most relevant features for your logistic regression model. The goal is to improve model accuracy, reduce overfitting, and enhance interpretability.\n",
    "\n",
    "**Why is it important?**\n",
    "- Reduces model complexity.\n",
    "- Improves model generalization.\n",
    "- Enhances model interpretability.\n",
    "\n",
    "**Code Example for Feature Selection:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Create a SelectKBest object\n",
    "k_best = SelectKBest(score_func=chi2, k=5)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_selected = k_best.fit_transform(X_train, y_train)\n",
    "X_test_selected = k_best.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddb47aa",
   "metadata": {},
   "source": [
    "## Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287344dd",
   "metadata": {},
   "source": [
    "Categorical variables need to be transformed into a numerical format for logistic regression models. Common encoding techniques include one-hot encoding or label encoding.\n",
    "\n",
    "**Why is it important?**\n",
    "- Allows the model to work with categorical data.\n",
    "- Ensures compatibility with logistic regression.\n",
    "\n",
    "**Code Example for One-Hot Encoding:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create a OneHotEncoder object\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_encoded = encoder.fit_transform(X_train_categorical)\n",
    "X_test_encoded = encoder.transform(X_test_categorical)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49541c",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe389d1b",
   "metadata": {},
   "source": [
    "Missing data can be problematic for logistic regression models. It's essential to decide whether to impute missing values or remove instances with missing data.\n",
    "\n",
    "**Why is it important?**\n",
    "- Avoids bias and errors in model predictions.\n",
    "- Maintains data quality and model performance.\n",
    "\n",
    "**Code Example for Missing Data Imputation:**\n",
    "\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a SimpleImputer object\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "```\n",
    "\n",
    "These code examples demonstrate the importance of feature selection, encoding categorical variables, and handling missing data for logistic regression. Effective feature engineering enhances the model's predictive power and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16576813",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb160237",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a critical step in optimizing the performance of logistic regression models. In this section, we will explore the importance of hyperparameter tuning and how to perform grid search or random search to find optimal hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87652925",
   "metadata": {},
   "source": [
    "## Importance of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c6a3",
   "metadata": {},
   "source": [
    "Hyperparameters are configuration settings that control the behavior of a logistic regression model. Tuning these hyperparameters can significantly impact the model's predictive power and generalization.\n",
    "\n",
    "**Why is it important?**\n",
    "- Improves model performance.\n",
    "- Avoids overfitting or underfitting.\n",
    "- Enhances the model's ability to capture complex patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193c46bb",
   "metadata": {},
   "source": [
    "## Grid Search and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dea2393",
   "metadata": {},
   "source": [
    "Two common methods for hyperparameter tuning are grid search and random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58da03f",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc23f8",
   "metadata": {},
   "source": [
    "Grid search involves specifying a set of hyperparameter values to test exhaustively. It systematically explores all possible combinations to find the best set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93dfc41",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ad9a4",
   "metadata": {},
   "source": [
    "Random search, on the other hand, randomly samples hyperparameters from predefined distributions. It provides a more efficient approach for tuning when computation resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c96ac4",
   "metadata": {},
   "source": [
    "## Code Examples for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fc939",
   "metadata": {},
   "source": [
    "Let's see how to perform hyperparameter tuning for both binary and multinomial logistic regression using grid search and random search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d542087",
   "metadata": {},
   "source": [
    "### Grid Search for Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076271c",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create a Binary Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the model to the training data with hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3e308",
   "metadata": {},
   "source": [
    "### Random Search for Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec70432",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define hyperparameters and their distribution\n",
    "param_dist = {\n",
    "    'C': uniform(0.001, 10),\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'l1_ratio': uniform(0, 1)\n",
    "}\n",
    "\n",
    "# Create a Multinomial Logistic Regression model\n",
    "multinomial_model = LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(multinomial_model, param_distributions=param_dist, cv=5, n_iter=20)\n",
    "\n",
    "# Fit the model to the training data with hyperparameter tuning\n",
    "random_search.fit(X_train_multinomial, y_train_multinomial)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params_multinomial = random_search.best_params_\n",
    "print(\"Best Hyperparameters (Multinomial):\", best_params_multinomial)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b1ef1",
   "metadata": {},
   "source": [
    "These code examples demonstrate the importance of hyperparameter tuning and how to perform grid search and random search to find the optimal hyperparameters for logistic regression models. Fine-tuning hyperparameters can significantly enhance the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85436ff5",
   "metadata": {},
   "source": [
    "# Real-Life Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05961744",
   "metadata": {},
   "source": [
    "Logistic regression finds applications in a wide range of real-life scenarios and industries. In this section, we will explore some common use cases where both binary and multinomial logistic regression are employed for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76018d6c",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1298ec",
   "metadata": {},
   "source": [
    "### Medical Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d96f7",
   "metadata": {},
   "source": [
    "**Analysis:** Binary logistic regression is used for medical diagnosis, such as identifying whether a patient has a particular disease (e.g., diabetes, heart disease) based on various clinical and diagnostic features.\n",
    "\n",
    "**Benefits:** It enables doctors to make informed decisions, provides early detection of diseases, and minimizes the risk of false diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef64612",
   "metadata": {},
   "source": [
    "### Credit Risk Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09649f",
   "metadata": {},
   "source": [
    "**Analysis:** Binary logistic regression helps assess credit risk by predicting whether a loan applicant is likely to default or not based on their financial history and other relevant factors.\n",
    "\n",
    "**Benefits:** It aids financial institutions in making lending decisions, managing risk, and reducing potential losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49660d9",
   "metadata": {},
   "source": [
    "### Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10743dd3",
   "metadata": {},
   "source": [
    "**Analysis:** Businesses use binary logistic regression to predict whether a customer is likely to churn (leave) or stay with their services based on historical usage and behavior.\n",
    "\n",
    "**Benefits:** It allows companies to proactively retain customers, optimize marketing strategies, and improve customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685bd620",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678bafb",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6f3631",
   "metadata": {},
   "source": [
    "**Analysis:** Multinomial logistic regression is applied in NLP for tasks like sentiment analysis and text classification, where text documents are categorized into multiple classes (e.g., positive, negative, neutral).\n",
    "\n",
    "**Benefits:** It helps automate the classification of vast amounts of text data, making it useful for social media sentiment analysis and content recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce176c9",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e30b9",
   "metadata": {},
   "source": [
    "**Analysis:** Multinomial logistic regression is used for image classification problems, where images are categorized into multiple classes (e.g., identifying objects or animals in images).\n",
    "\n",
    "**Benefits:** It finds applications in image recognition systems, autonomous vehicles, and medical image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd5186",
   "metadata": {},
   "source": [
    "### Species Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266fc52",
   "metadata": {},
   "source": [
    "**Analysis:** Multinomial logistic regression can be employed in the classification of species in biology and environmental science, based on various features or characteristics.\n",
    "\n",
    "**Benefits:** It aids in biodiversity assessment, conservation efforts, and ecological research.\n",
    "\n",
    "In each of these use cases, logistic regression allows for the classification of data into different categories, providing valuable insights and predictive capabilities. The choice between binary and multinomial logistic regression depends on the specific problem and the nature of the classes involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d0398",
   "metadata": {},
   "source": [
    "# Content Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7345788",
   "metadata": {},
   "source": [
    "**Introduction and Background**\n",
    "\n",
    "In this notebook, we embarked on a comprehensive journey into the world of Logistic Regression, particularly focusing on Binary and Multinomial Logistic Regression. We learned that logistic regression is a popular classification algorithm widely used in various fields, with a natural fit for binary and multi-class classification problems.\n",
    "\n",
    "**Binary Logistic Regression**\n",
    "\n",
    "We delved into Binary Logistic Regression, understanding its purpose, the log-odds and sigmoid function, and the importance of coefficients. We explored use cases in medical diagnosis, credit risk assessment, and customer churn prediction, highlighting its significance in decision-making processes.\n",
    "\n",
    "**Multinomial Logistic Regression**\n",
    "\n",
    "Multinomial Logistic Regression was our next destination. We learned about the softmax function, model training, and complex decision boundaries. We explored applications in NLP, image classification, and species classification, emphasizing its versatility in handling multi-class classification tasks.\n",
    "\n",
    "**Code Examples**\n",
    "\n",
    "The notebook provided practical code examples for both Binary and Multinomial Logistic Regression. We covered data loading, model fitting, interpretation of results, and model evaluation, showcasing the hands-on aspects of logistic regression implementation.\n",
    "\n",
    "**Feature Engineering**\n",
    "\n",
    "Feature engineering was discussed in the context of logistic regression, including feature selection, encoding categorical variables, and handling missing data. We understood the crucial role these steps play in improving model performance.\n",
    "\n",
    "**Hyperparameter Tuning**\n",
    "\n",
    "Hyperparameter tuning was explored, demonstrating the importance of fine-tuning logistic regression models. Grid search and random search methods were introduced, along with code examples for optimizing hyperparameters.\n",
    "\n",
    "**Real-Life Use Cases**\n",
    "\n",
    "The real-life use cases of logistic regression were presented, spanning medical diagnosis, credit risk assessment, customer churn prediction, NLP, image classification, and species classification. We highlighted the analysis conducted in each case and the benefits of using logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09c7ef",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ffb75",
   "metadata": {},
   "source": [
    "In conclusion, this notebook has equipped you with a solid understanding of Binary and Multinomial Logistic Regression, from theory to practical implementation. Logistic regression is a powerful tool for classification tasks and finds its applications in diverse fields, making it a valuable addition to your data science and machine learning toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
