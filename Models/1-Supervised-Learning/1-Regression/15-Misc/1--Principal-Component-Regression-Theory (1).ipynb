{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e15e28",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Principal-Component-Regression-(PCR)\" data-toc-modified-id=\"Principal-Component-Regression-(PCR)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Principal Component Regression (PCR)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction-and-Background\" data-toc-modified-id=\"Introduction-and-Background-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction and Background</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-PCR?\" data-toc-modified-id=\"What-is-PCR?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What is PCR?</a></span></li><li><span><a href=\"#The-Purpose-of-PCR\" data-toc-modified-id=\"The-Purpose-of-PCR-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>The Purpose of PCR</a></span></li><li><span><a href=\"#Applications-of-PCR\" data-toc-modified-id=\"Applications-of-PCR-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Applications of PCR</a></span></li></ul></li></ul></li><li><span><a href=\"#Introduction-to-Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Introduction-to-Principal-Component-Analysis-(PCA)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction to Principal Component Analysis (PCA)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-Dimensionality-Reduction\" data-toc-modified-id=\"Understanding-Dimensionality-Reduction-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Understanding Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Curse-of-Dimensionality\" data-toc-modified-id=\"The-Curse-of-Dimensionality-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>The Curse of Dimensionality</a></span></li></ul></li><li><span><a href=\"#The-Main-Idea\" data-toc-modified-id=\"The-Main-Idea-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>The Main Idea</a></span><ul class=\"toc-item\"><li><span><a href=\"#Eigenvectors-and-Eigenvalues\" data-toc-modified-id=\"Eigenvectors-and-Eigenvalues-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Eigenvectors and Eigenvalues</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Dimensionality Reduction</a></span></li></ul></li><li><span><a href=\"#Need-for-Dimensionality-Reduction\" data-toc-modified-id=\"Need-for-Dimensionality-Reduction-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Need for Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Importance-of-Dimensionality-Reduction\" data-toc-modified-id=\"Importance-of-Dimensionality-Reduction-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Importance of Dimensionality Reduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenges-of-High-Dimensional-Datasets\" data-toc-modified-id=\"Challenges-of-High-Dimensional-Datasets-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Challenges of High-Dimensional Datasets</a></span></li></ul></li><li><span><a href=\"#Dimensionality-Reduction-in-Regression\" data-toc-modified-id=\"Dimensionality-Reduction-in-Regression-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Dimensionality Reduction in Regression</a></span></li></ul></li><li><span><a href=\"#PCR-Methodology\" data-toc-modified-id=\"PCR-Methodology-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>PCR Methodology</a></span><ul class=\"toc-item\"><li><span><a href=\"#Understanding-Principal-Component-Regression-(PCR)\" data-toc-modified-id=\"Understanding-Principal-Component-Regression-(PCR)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Understanding Principal Component Regression (PCR)</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-Two-Step-Process\" data-toc-modified-id=\"The-Two-Step-Process-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>The Two-Step Process</a></span></li><li><span><a href=\"#The-Role-of-Principal-Components\" data-toc-modified-id=\"The-Role-of-Principal-Components-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>The Role of Principal Components</a></span></li><li><span><a href=\"#Advantages-of-PCR\" data-toc-modified-id=\"Advantages-of-PCR-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Advantages of PCR</a></span></li></ul></li><li><span><a href=\"#Code-Examples-for-PCR\" data-toc-modified-id=\"Code-Examples-for-PCR-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Code Examples for PCR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementing-Principal-Component-Regression-(PCR)-in-Python\" data-toc-modified-id=\"Implementing-Principal-Component-Regression-(PCR)-in-Python-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Implementing Principal Component Regression (PCR) in Python</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-Evaluation-for-PCR\" data-toc-modified-id=\"Model-Evaluation-for-PCR-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Evaluation for PCR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-the-Performance-of-PCR-Models\" data-toc-modified-id=\"Evaluating-the-Performance-of-PCR-Models-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Evaluating the Performance of PCR Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluating-Model-Fit\" data-toc-modified-id=\"Evaluating-Model-Fit-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Evaluating Model Fit</a></span></li><li><span><a href=\"#Assessing-the-Number-of-Principal-Components\" data-toc-modified-id=\"Assessing-the-Number-of-Principal-Components-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Assessing the Number of Principal Components</a></span></li></ul></li></ul></li><li><span><a href=\"#Choosing-the-Number-of-Principal-Components\" data-toc-modified-id=\"Choosing-the-Number-of-Principal-Components-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Choosing the Number of Principal Components</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Cross-Validation</a></span></li><li><span><a href=\"#Explained-Variance-Ratio\" data-toc-modified-id=\"Explained-Variance-Ratio-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Explained Variance Ratio</a></span><ul class=\"toc-item\"><li><span><a href=\"#Balancing-Complexity-and-Performance\" data-toc-modified-id=\"Balancing-Complexity-and-Performance-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Balancing Complexity and Performance</a></span></li></ul></li></ul></li><li><span><a href=\"#Advantages-and-Limitations-of-PCR\" data-toc-modified-id=\"Advantages-and-Limitations-of-PCR-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Advantages and Limitations of PCR</a></span><ul class=\"toc-item\"><li><span><a href=\"#Advantages-of-PCR\" data-toc-modified-id=\"Advantages-of-PCR-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Advantages of PCR</a></span></li><li><span><a href=\"#Limitations-of-PCR\" data-toc-modified-id=\"Limitations-of-PCR-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Limitations of PCR</a></span></li></ul></li><li><span><a href=\"#Real-Life-Use-Cases\" data-toc-modified-id=\"Real-Life-Use-Cases-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Real-Life Use Cases</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finance-and-Economics\" data-toc-modified-id=\"Finance-and-Economics-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Finance and Economics</a></span></li><li><span><a href=\"#Chemistry-and-Spectroscopy\" data-toc-modified-id=\"Chemistry-and-Spectroscopy-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Chemistry and Spectroscopy</a></span></li><li><span><a href=\"#Medicine-and-Healthcare\" data-toc-modified-id=\"Medicine-and-Healthcare-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Medicine and Healthcare</a></span></li><li><span><a href=\"#Environmental-Science\" data-toc-modified-id=\"Environmental-Science-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Environmental Science</a></span></li><li><span><a href=\"#Marketing-and-Customer-Analytics\" data-toc-modified-id=\"Marketing-and-Customer-Analytics-8.5\"><span class=\"toc-item-num\">8.5&nbsp;&nbsp;</span>Marketing and Customer Analytics</a></span></li><li><span><a href=\"#Manufacturing-and-Quality-Control\" data-toc-modified-id=\"Manufacturing-and-Quality-Control-8.6\"><span class=\"toc-item-num\">8.6&nbsp;&nbsp;</span>Manufacturing and Quality Control</a></span></li></ul></li><li><span><a href=\"#Content-Summarization\" data-toc-modified-id=\"Content-Summarization-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Content Summarization</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7a940",
   "metadata": {},
   "source": [
    "# Principal Component Regression (PCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e300a8",
   "metadata": {},
   "source": [
    "## Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcad5f",
   "metadata": {},
   "source": [
    "Principal Component Regression (PCR) is a powerful statistical technique that combines elements of Principal Component Analysis (PCA) and linear regression. It's used in data analysis and modeling to address challenges related to high-dimensional datasets, multicollinearity, and dimensionality reduction. PCR can be a valuable tool in various fields, including statistics, machine learning, and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73892e34",
   "metadata": {},
   "source": [
    "### What is PCR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34146b",
   "metadata": {},
   "source": [
    "PCR is a hybrid approach that leverages the strengths of both PCA and linear regression. It begins with PCA, a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional representation by identifying and retaining the most informative components (principal components). These components are linear combinations of the original variables.\n",
    "\n",
    "Once the principal components are obtained, PCR applies linear regression to predict the target variable using the reduced set of principal components as predictors. This two-step process combines dimensionality reduction and regression modeling to enhance predictive performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe9033",
   "metadata": {},
   "source": [
    "### The Purpose of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a49cf3",
   "metadata": {},
   "source": [
    "PCR serves several important purposes in data analysis and modeling:\n",
    "\n",
    "1. **Dimensionality Reduction:** High-dimensional datasets often contain redundant or irrelevant features. PCR reduces dimensionality by retaining only the most important components while discarding the noise.\n",
    "\n",
    "2. **Multicollinearity Mitigation:** In traditional linear regression, multicollinearity (high correlation between predictor variables) can lead to unstable coefficient estimates. PCR mitigates this issue by using uncorrelated principal components as predictors.\n",
    "\n",
    "3. **Interpretability:** PCR makes regression models more interpretable, as the relationship between the target variable and principal components is often simpler and easier to understand.\n",
    "\n",
    "4. **Overfitting Reduction:** By reducing the number of predictors, PCR helps prevent overfitting, especially in cases where the number of features is close to or exceeds the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5baf5",
   "metadata": {},
   "source": [
    "### Applications of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95e9aa",
   "metadata": {},
   "source": [
    "PCR finds applications in a wide range of fields:\n",
    "\n",
    "- **Chemometrics:** In analytical chemistry, PCR is used for spectral analysis, allowing the identification and quantification of chemical compounds in complex mixtures.\n",
    "- **Bioinformatics:** PCR is applied in genomics and proteomics to extract valuable information from high-dimensional biological data.\n",
    "- **Economics:** In econometrics, PCR is used for modeling and forecasting economic data with many variables.\n",
    "- **Machine Learning:** PCR can be an essential preprocessing step in machine learning tasks to reduce data dimensionality and improve model performance.\n",
    "- **Quality Control:** In manufacturing and quality control, PCR helps identify patterns and relationships in multivariate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8b31b",
   "metadata": {},
   "source": [
    "# Introduction to Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32725f54",
   "metadata": {},
   "source": [
    "## Understanding Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6591883f",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a pivotal technique in the field of multivariate data analysis. It plays a central role in addressing the challenges posed by high-dimensional datasets, a common occurrence in data analysis and machine learning. PCA is the foundational step in Principal Component Regression (PCR) and is used to reduce the dimensionality of data while retaining as much information as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8527ea4",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee927b89",
   "metadata": {},
   "source": [
    "High-dimensional datasets are characterized by a large number of variables or features. While these datasets contain valuable information, their high dimensionality poses several challenges:\n",
    "\n",
    "- **Overfitting:** In traditional regression models, a high number of predictors relative to observations can lead to overfitting, where the model fits noise in the data, making it perform poorly on new data.\n",
    "\n",
    "- **Computational Complexity:** As the number of features increases, the computational resources required to process and analyze the data grow substantially.\n",
    "\n",
    "- **Visualization Difficulty:** Visualizing data in high-dimensional space is nearly impossible, making it challenging to gain insights from data exploration.\n",
    "\n",
    "- **Multicollinearity:** High-dimensional data often contains correlated variables, leading to multicollinearity, which can destabilize regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255a216",
   "metadata": {},
   "source": [
    "## The Main Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe9253",
   "metadata": {},
   "source": [
    "The core concept of PCA is to transform the original high-dimensional data into a new coordinate system, where the axes (principal components) are linear combinations of the original features. These principal components are ordered in terms of the amount of variance they explain in the data. By focusing on a subset of the principal components, we can effectively reduce the dimensionality of the dataset while retaining most of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26c6a5",
   "metadata": {},
   "source": [
    "### Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f485794c",
   "metadata": {},
   "source": [
    "PCA relies on two fundamental linear algebra concepts:\n",
    "\n",
    "- **Eigenvectors:** Eigenvectors represent the directions along which the data varies the most. In the context of PCA, each eigenvector corresponds to a principal component. These vectors are orthogonal to each other, meaning they capture independent directions of variance.\n",
    "\n",
    "- **Eigenvalues:** Eigenvalues are associated with each eigenvector and represent the amount of variance explained by that component. High eigenvalues indicate that the corresponding principal component captures a substantial portion of the data's variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b215a5f",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9fafb",
   "metadata": {},
   "source": [
    "After computing the principal components, the next step is to determine how many of them to retain. This is a crucial decision that balances the desire for dimensionality reduction with the need to preserve as much of the original information as possible. \n",
    "\n",
    "By selecting a subset of the top $k$ principal components, where $k$ is typically much smaller than the original number of features, we create a lower-dimensional representation of the data. This reduced representation is used for various purposes, such as visualization, feature selection, and as predictors in regression models like PCR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd525f6",
   "metadata": {},
   "source": [
    "# Need for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd755736",
   "metadata": {},
   "source": [
    "## Importance of Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4eef2",
   "metadata": {},
   "source": [
    "In the realm of data analysis and regression modeling, the need for dimensionality reduction becomes evident when dealing with high-dimensional datasets. Dimensionality reduction is the process of reducing the number of variables (features or predictors) in a dataset while preserving essential information. This process is instrumental in mitigating several challenges associated with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b2709",
   "metadata": {},
   "source": [
    "### Challenges of High-Dimensional Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21c544",
   "metadata": {},
   "source": [
    "High-dimensional datasets, characterized by a large number of variables, present a host of challenges that can impact the effectiveness of regression models and data analysis in general. Here are some of the key challenges:\n",
    "\n",
    "1. **Overfitting:** In traditional regression models, where the number of predictors exceeds the number of observations, overfitting is a common problem. Overfit models capture noise in the data, leading to poor generalization to new, unseen data. Dimensionality reduction helps prevent overfitting by reducing the number of predictors.\n",
    "\n",
    "2. **Computational Complexity:** As the dimensionality of the dataset increases, the computational resources required for processing and analyzing the data grow exponentially. This results in longer computation times and increased demands on hardware resources. Dimensionality reduction can significantly alleviate computational burdens.\n",
    "\n",
    "3. **Visualization Difficulty:** High-dimensional spaces are difficult to visualize and comprehend. Data visualization is an essential tool for understanding patterns, relationships, and outliers in the data. Reducing dimensionality allows for more effective data visualization, making it easier to gain insights from the data.\n",
    "\n",
    "4. **Curse of Dimensionality:** High-dimensional spaces can exhibit counterintuitive properties. For example, data points in high-dimensional space are often sparse and distant from each other, leading to increased distance between data points and making similarity-based algorithms less effective. Dimensionality reduction can mitigate the curse of dimensionality.\n",
    "\n",
    "5. **Multicollinearity:** High-dimensional datasets often contain highly correlated variables, leading to multicollinearity. Multicollinearity can destabilize regression models and lead to unstable coefficient estimates. Dimensionality reduction can help reduce multicollinearity.\n",
    "\n",
    "6. **Interpretability:** High-dimensional models can be challenging to interpret. The relationship between the predictors and the target variable becomes complex, and the role of individual predictors may be less clear. Reducing dimensionality often leads to simpler and more interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bbadb0",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dfe7c",
   "metadata": {},
   "source": [
    "In the context of regression modeling, dimensionality reduction is particularly important. The reduction of predictor variables not only simplifies the model but can also improve predictive accuracy by removing irrelevant or redundant features.\n",
    "\n",
    "Principal Component Regression (PCR) addresses the challenges of high-dimensional datasets by using Principal Component Analysis (PCA) to create a lower-dimensional representation of the data. By selecting a subset of the most informative principal components, PCR simplifies regression models, enhances interpretability, and can lead to better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea598e",
   "metadata": {},
   "source": [
    "# PCR Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eacd6c",
   "metadata": {},
   "source": [
    "## Understanding Principal Component Regression (PCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02df239",
   "metadata": {},
   "source": [
    "Principal Component Regression (PCR) is a powerful statistical technique that seamlessly combines two essential components: Principal Component Analysis (PCA) and linear regression. This methodology offers a solution to the challenges posed by high-dimensional datasets and multicollinearity in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28637d7",
   "metadata": {},
   "source": [
    "### The Two-Step Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbc109",
   "metadata": {},
   "source": [
    "PCR is characterized by a two-step process:\n",
    "\n",
    "1. **Principal Component Analysis (PCA):** In the initial step, PCR applies PCA to the original predictor variables. PCA transforms the high-dimensional feature space into a new set of orthogonal axes, known as principal components. These components are linear combinations of the original features and are ordered by the amount of variance they capture in the data. The first principal component explains the most variance, the second explains the second most, and so on. \n",
    "\n",
    "   The principal components are designed to be uncorrelated with each other, meaning they provide a way to eliminate multicollinearity among predictors. This transformation of the data simplifies the regression problem and reduces the dimensionality of the dataset.\n",
    "\n",
    "2. **Linear Regression on Principal Components:** In the second step, PCR applies linear regression to predict the target variable using the retained principal components as predictors. By focusing on these components, PCR leverages the dimensionality reduction benefits of PCA while maintaining a regression framework. This simplifies the interpretation of the model and enhances its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6a1c0",
   "metadata": {},
   "source": [
    "### The Role of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5681129",
   "metadata": {},
   "source": [
    "The key idea in PCR is that the principal components retain most of the information in the data while reducing dimensionality and multicollinearity. These components capture the essential patterns and relationships in the data, allowing for a more concise and interpretable regression model.\n",
    "\n",
    "By selecting a subset of the principal components (often those that explain the most variance), PCR retains the most informative directions in the feature space, effectively reducing the number of predictors. This reduction helps mitigate the risk of overfitting, making the model more robust and better suited for high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbc78e",
   "metadata": {},
   "source": [
    "### Advantages of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8917fe9",
   "metadata": {},
   "source": [
    "PCR offers several advantages in regression modeling:\n",
    "\n",
    "- **Dimensionality Reduction:** By focusing on the most informative principal components, PCR significantly reduces the number of predictors, simplifying the model.\n",
    "\n",
    "- **Multicollinearity Mitigation:** The uncorrelated nature of the principal components addresses multicollinearity, ensuring stable coefficient estimates.\n",
    "\n",
    "- **Interpretability:** PCR results in a more interpretable model, as the relationship between the target variable and the principal components is often simpler to understand.\n",
    "\n",
    "- **Overfitting Prevention:** Dimensionality reduction helps prevent overfitting, especially when the number of features exceeds the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584d5bc",
   "metadata": {},
   "source": [
    "## Code Examples for PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f016859",
   "metadata": {},
   "source": [
    "### Implementing Principal Component Regression (PCR) in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523b577",
   "metadata": {},
   "source": [
    "The example will cover the following steps:\n",
    "\n",
    "1. Loading the dataset.\n",
    "2. Applying Principal Component Analysis (PCA) to reduce dimensionality.\n",
    "3. Fitting a linear regression model on the principal components.\n",
    "4. Evaluating the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d2104",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (replace 'dataset.csv' with your dataset file)\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Separate predictors and the target variable\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "n_components = 10  # Number of principal components to retain\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Fit a linear regression model on the principal components\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = regression_model.predict(X_test_pca)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b628f9",
   "metadata": {},
   "source": [
    "In this example, we first load the dataset, separate the predictors (X) and the target variable (y), and then split the data into training and testing sets. We apply PCA to reduce dimensionality, specifying the number of principal components to retain (in this case, 10). The regression model is then fitted on the principal components, and predictions are made on the test set. We evaluate the model's performance using Mean Squared Error (MSE).\n",
    "\n",
    "This code example demonstrates the steps involved in implementing PCR and showcases how dimensionality reduction can enhance regression modeling on high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67331",
   "metadata": {},
   "source": [
    "# Model Evaluation for PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a39ce",
   "metadata": {},
   "source": [
    "## Evaluating the Performance of PCR Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14dde7",
   "metadata": {},
   "source": [
    "After implementing Principal Component Regression (PCR), it's crucial to evaluate the performance of the model. Model evaluation helps assess how well the PCR model fits the data, making it easier to determine whether the selected number of principal components is appropriate and whether the model performs effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e45c1",
   "metadata": {},
   "source": [
    "### Evaluating Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d135b",
   "metadata": {},
   "source": [
    "1. **Mean Squared Error (MSE):** MSE is a common metric for regression models, including PCR. It measures the average squared difference between the predicted values and the actual target values. A lower MSE indicates a better fit. Calculate MSE for both the training and testing datasets to check for overfitting.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate MSE for training set\n",
    "y_train_pred = regression_model.predict(X_train_pca)\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate MSE for testing set\n",
    "y_test_pred = regression_model.predict(X_test_pca)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training MSE: {mse_train:.2f}\")\n",
    "print(f\"Testing MSE: {mse_test:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b06d17",
   "metadata": {},
   "source": [
    "2. **R-squared (R2):** R-squared measures the proportion of the variance in the target variable that is predictable from the predictors. A higher R-squared indicates a better fit. Calculate R2 for both the training and testing datasets.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R2 for training set\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Calculate R2 for testing set\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training R2: {r2_train:.2f}\")\n",
    "print(f\"Testing R2: {r2_test:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8f0f7",
   "metadata": {},
   "source": [
    "### Assessing the Number of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28617298",
   "metadata": {},
   "source": [
    "Selecting the right number of principal components is crucial in PCR. An excessive number of components may lead to overfitting, while too few may result in information loss. To assess the number of principal components to retain, you can use the explained variance ratio provided by PCA.\n",
    "\n",
    "3. **Explained Variance Ratio:** This metric tells you the proportion of the total variance in the data explained by each principal component. Plotting the cumulative explained variance ratio can help determine the optimal number of components to retain.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Create a scree plot to visualize the cumulative explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, n_components + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd927a06",
   "metadata": {},
   "source": [
    "In the scree plot, look for an \"elbow point\" where adding more components does not significantly increase the explained variance. This point can guide your decision on the optimal number of principal components to retain.\n",
    "\n",
    "Evaluating PCR models involves a combination of model fit assessment using metrics like MSE and R-squared and determining the number of principal components to retain based on the explained variance ratio. These steps help ensure your PCR model is both effective and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e9264",
   "metadata": {},
   "source": [
    "# Choosing the Number of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e725ac",
   "metadata": {},
   "source": [
    "One of the key decisions in Principal Component Regression (PCR) is determining the optimal number of principal components to retain. This choice significantly impacts the model's performance, interpretability, and ability to address the challenges of high-dimensional data. Several methods can help you decide on the right number of components to include."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e4b5e",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f2846",
   "metadata": {},
   "source": [
    "Cross-validation is a widely used technique for selecting the number of principal components in PCR. It involves splitting the data into multiple subsets (folds), using some for training and others for validation. By repeating this process with different numbers of components, you can identify the number that leads to the best model performance.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define a function to calculate cross-validated MSE\n",
    "def cross_val_mse(n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    regression_model = LinearRegression()\n",
    "    scores = -cross_val_score(regression_model, X_train_pca, y_train, scoring=\"neg_mean_squared_error\", cv=5)\n",
    "    return scores.mean()\n",
    "\n",
    "# Test different numbers of components and find the optimal one\n",
    "components_range = range(1, 21)\n",
    "mse_scores = [cross_val_mse(n) for n in components_range]\n",
    "\n",
    "optimal_n_components = components_range[np.argmin(mse_scores)]\n",
    "print(f\"Optimal Number of Components: {optimal_n_components}\")\n",
    "```\n",
    "\n",
    "In the code above, we perform cross-validation to compute the mean squared error (MSE) for different numbers of principal components. The optimal number is selected based on the component count that minimizes the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1314378",
   "metadata": {},
   "source": [
    "## Explained Variance Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9dded7",
   "metadata": {},
   "source": [
    "Another method for selecting the number of principal components is examining the explained variance ratio. This metric indicates the proportion of the total variance in the data that each principal component explains. A common practice is to retain a sufficient number of components to capture a significant portion of the data's variance.\n",
    "\n",
    "```python\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components that capture a desired variance threshold (e.g., 95%)\n",
    "desired_variance_threshold = 0.95\n",
    "n_components_to_retain = np.argmax(cumulative_variance >= desired_variance_threshold) + 1\n",
    "print(f\"Number of Components to Retain for {desired_variance_threshold*100}% Variance: {n_components_to_retain}\")\n",
    "```\n",
    "\n",
    "By analyzing the cumulative explained variance ratio, you can determine the number of components that capture a specified portion of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16f3f5",
   "metadata": {},
   "source": [
    "### Balancing Complexity and Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc20d9",
   "metadata": {},
   "source": [
    "The choice of the number of principal components should strike a balance between model complexity and performance. More components provide a richer representation of the data but can lead to overfitting. Fewer components reduce the risk of overfitting but may result in information loss. Cross-validation and explained variance analysis help you make an informed decision based on your specific modeling goals and data characteristics.\n",
    "\n",
    "Selecting the optimal number of principal components is a critical step in PCR, ensuring that the model effectively reduces dimensionality, mitigates multicollinearity, and maintains predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7648f6",
   "metadata": {},
   "source": [
    "# Advantages and Limitations of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e5d85",
   "metadata": {},
   "source": [
    "## Advantages of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8be315",
   "metadata": {},
   "source": [
    "Principal Component Regression (PCR) offers several advantages that make it a valuable technique in data analysis and regression modeling:\n",
    "\n",
    "1. **Dimensionality Reduction:** PCR effectively reduces the dimensionality of high-dimensional datasets. By focusing on a subset of informative principal components, it simplifies the model, making it computationally more efficient and less prone to overfitting.\n",
    "\n",
    "2. **Multicollinearity Mitigation:** High-dimensional datasets often contain correlated predictor variables, which can lead to multicollinearity in regression models. PCR addresses this issue by using principal components, which are orthogonal and uncorrelated, as predictors. This ensures more stable coefficient estimates.\n",
    "\n",
    "3. **Interpretability:** The relationship between the target variable and the principal components is often simpler and more interpretable than the relationship between the target variable and the original predictors. This makes PCR a useful tool for understanding the impact of variables on the target.\n",
    "\n",
    "4. **Overfitting Prevention:** Reducing the number of predictors through PCA helps prevent overfitting, especially when the number of features is close to or exceeds the number of observations. PCR provides a balance between model complexity and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc68ae6",
   "metadata": {},
   "source": [
    "## Limitations of PCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cfe47e",
   "metadata": {},
   "source": [
    "While PCR offers substantial advantages, it also has certain limitations that should be considered:\n",
    "\n",
    "1. **Interpretation Challenge:** One of the main limitations of PCR is the challenge of interpreting the coefficients in terms of the original variables. Since the predictors in the model are principal components, it can be challenging to relate them back to the original features. This makes it less intuitive to understand the practical implications of changes in the predictors.\n",
    "\n",
    "2. **Information Loss:** In the process of dimensionality reduction, some information is inevitably lost. By retaining only a subset of principal components, you may discard some variability in the data. Care must be taken to strike the right balance between dimensionality reduction and information preservation.\n",
    "\n",
    "3. **Assumption of Linearity:** PCR assumes a linear relationship between the target variable and the retained principal components. If the true relationship is highly nonlinear, PCR may not capture it effectively. In such cases, more advanced regression techniques may be more suitable.\n",
    "\n",
    "4. **Model Complexity:** While PCR simplifies the regression model by reducing the number of predictors, it introduces a new set of variables (principal components). Selecting the optimal number of components remains a critical task and can affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6d934",
   "metadata": {},
   "source": [
    "Understanding these advantages and limitations is essential when deciding whether to use PCR in a given data analysis or regression modeling scenario. PCR is a powerful technique, but its suitability depends on the specific characteristics of the data and the modeling objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897cfbf",
   "metadata": {},
   "source": [
    "# Real-Life Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fd525",
   "metadata": {},
   "source": [
    "Principal Component Regression (PCR) finds extensive use in various real-life applications and industry domains. Here, we explore some common use cases where PCR is employed to address specific challenges and deliver valuable insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18092817",
   "metadata": {},
   "source": [
    "## Finance and Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ebe48",
   "metadata": {},
   "source": [
    "**Type of Analysis:** In the financial sector, PCR is used to analyze economic data, predict stock prices, and build risk models.\n",
    "\n",
    "**Benefits of PCR:** PCR helps reduce multicollinearity among financial indicators, making it easier to assess the impact of various factors on stock prices and economic trends. It simplifies complex financial models while preserving predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e8394",
   "metadata": {},
   "source": [
    "## Chemistry and Spectroscopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b64c9a",
   "metadata": {},
   "source": [
    "**Type of Analysis:** PCR is used in chemistry to analyze spectral data, such as infrared or nuclear magnetic resonance (NMR) spectra, to predict chemical properties.\n",
    "\n",
    "**Benefits of PCR:** Spectral data often contain a large number of variables with inherent multicollinearity. PCR simplifies the analysis by transforming spectra into principal components, making it easier to relate spectral patterns to chemical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f32a1",
   "metadata": {},
   "source": [
    "## Medicine and Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce8bc41",
   "metadata": {},
   "source": [
    "**Type of Analysis:** In the medical field, PCR is employed to analyze patient data and predict outcomes, such as disease progression or the effectiveness of treatments.\n",
    "\n",
    "**Benefits of PCR:** PCR is used to handle high-dimensional medical data, where numerous health indicators and genomic variables may be interrelated. It simplifies models, making them more interpretable and facilitating predictions and treatment recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec27abf",
   "metadata": {},
   "source": [
    "## Environmental Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108514be",
   "metadata": {},
   "source": [
    "**Type of Analysis:** PCR is applied to environmental data to predict factors like air quality, climate change, or pollutant levels.\n",
    "\n",
    "**Benefits of PCR:** Environmental datasets are often high-dimensional and include interrelated variables. PCR helps build predictive models that can be used for early detection and intervention in environmental issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3bdc3d",
   "metadata": {},
   "source": [
    "## Marketing and Customer Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b29fb1",
   "metadata": {},
   "source": [
    "**Type of Analysis:** In marketing, PCR is used to analyze customer data, predict purchase behavior, and optimize advertising campaigns.\n",
    "\n",
    "**Benefits of PCR:** Customer datasets can be vast, including demographic, behavioral, and transactional data. PCR simplifies models, making them more efficient and allowing marketers to identify the key drivers of customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1235cc4b",
   "metadata": {},
   "source": [
    "## Manufacturing and Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9699688",
   "metadata": {},
   "source": [
    "**Type of Analysis:** PCR is used in manufacturing to analyze data related to product quality, process control, and defect prediction.\n",
    "\n",
    "**Benefits of PCR:** Manufacturing processes often generate a wealth of data, including sensor readings and process variables. PCR simplifies the analysis, making it easier to identify factors affecting product quality and process efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e839225e",
   "metadata": {},
   "source": [
    "These real-life use cases highlight the versatility of Principal Component Regression in addressing the challenges of high-dimensional data, multicollinearity, and model interpretability. PCR is a valuable tool in various domains, enabling analysts and professionals to extract meaningful insights and make data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c756f",
   "metadata": {},
   "source": [
    "# Content Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a04b8b",
   "metadata": {},
   "source": [
    "**1. Introduction and Background:**\n",
    "   - PCR combines Principal Component Analysis (PCA) and linear regression.\n",
    "   - Essential for addressing high-dimensional data challenges.\n",
    "\n",
    "**2. Introduction to Principal Component Analysis (PCA):**\n",
    "   - PCA transforms high-dimensional data into orthogonal principal components.\n",
    "   - Eigenvectors and eigenvalues are fundamental concepts.\n",
    "   - Reducing dimensionality while retaining information is a key goal.\n",
    "\n",
    "**3. Need for Dimensionality Reduction:**\n",
    "   - High-dimensional datasets present challenges like overfitting and computational complexity.\n",
    "   - Dimensionality reduction simplifies models and enhances data visualization.\n",
    "\n",
    "**4. PCR Methodology:**\n",
    "   - PCR combines PCA and linear regression in a two-step process.\n",
    "   - Principal components simplify models and mitigate multicollinearity.\n",
    "   - Benefits include dimensionality reduction, interpretability, and overfitting prevention.\n",
    "\n",
    "**5. Code Examples for PCR:**\n",
    "   - Demonstrated PCR implementation using scikit-learn.\n",
    "   - Covered data loading, PCA, model fitting, and performance evaluation.\n",
    "\n",
    "**6. Model Evaluation for PCR:**\n",
    "   - Explained model evaluation with MSE and R-squared.\n",
    "   - Discussed assessing the number of principal components to retain.\n",
    "\n",
    "**7. Choosing the Number of Principal Components:**\n",
    "   - Explored methods like cross-validation and explained variance for component selection.\n",
    "   - Emphasized balancing model complexity and performance.\n",
    "\n",
    "**8. Advantages and Limitations of PCR:**\n",
    "   - Highlighted advantages, including dimensionality reduction and multicollinearity mitigation.\n",
    "   - Discussed limitations like coefficient interpretation and information loss.\n",
    "\n",
    "**9. Real-Life Use Cases:**\n",
    "   - Showcased applications in finance, chemistry, healthcare, environment, marketing, and manufacturing.\n",
    "   - Emphasized benefits of using PCR for data analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19564a84",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad4f09",
   "metadata": {},
   "source": [
    "- **Introduction:** PCR is a powerful technique combining PCA and linear regression to address high-dimensional data challenges.\n",
    "\n",
    "- **PCA Fundamentals:** PCA reduces dimensionality by transforming data into orthogonal principal components, utilizing eigenvectors and eigenvalues.\n",
    "\n",
    "- **Need for Dimensionality Reduction:** High-dimensional data poses challenges, and dimensionality reduction aids in model simplification and data visualization.\n",
    "\n",
    "- **PCR Methodology:** PCR's two-step process simplifies models, mitigates multicollinearity, and prevents overfitting.\n",
    "\n",
    "- **Code Examples:** Practical PCR implementation demonstrated through Python code examples using scikit-learn.\n",
    "\n",
    "- **Model Evaluation:** Model fit assessment using MSE and R-squared, and methods for selecting the number of principal components.\n",
    "\n",
    "- **Advantages and Limitations:** PCR offers dimensionality reduction and multicollinearity mitigation but challenges interpretation.\n",
    "\n",
    "- **Real-Life Use Cases:** PCR finds applications in various fields, simplifying data analysis and providing valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92520043",
   "metadata": {},
   "source": [
    "In conclusion, PCR is a versatile tool for dimensionality reduction and regression modeling. As you explore the world of data analysis and machine learning, consider delving into more advanced dimensionality reduction techniques and regression methods for even greater insights and model performance. PCR serves as an essential building block in this journey, simplifying complex data and revealing meaningful patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92af05c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
