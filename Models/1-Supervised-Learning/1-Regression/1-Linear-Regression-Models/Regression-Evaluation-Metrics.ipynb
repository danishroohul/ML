{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8708791b",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Error-Metrics\" data-toc-modified-id=\"Error-Metrics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Error Metrics</a></span></li><li><span><a href=\"#Goodness-of-Fit-Metrics\" data-toc-modified-id=\"Goodness-of-Fit-Metrics-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Goodness-of-Fit Metrics</a></span></li><li><span><a href=\"#Residual-Analysis-Metrics\" data-toc-modified-id=\"Residual-Analysis-Metrics-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Residual Analysis Metrics</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e05fb",
   "metadata": {},
   "source": [
    "**Error Metrics:**\n",
    "\n",
    "1. Mean Squared Error (MSE)\n",
    "2. Mean Absolute Error (MAE)\n",
    "3. Root Mean Squared Error (RMSE)\n",
    "4. Mean Absolute Percentage Error (MAPE)\n",
    "5. Median Absolute Percentage Error (MdAPE)\n",
    "6. Symmetric Mean Absolute Percentage Error (SMAPE)\n",
    "7. Geometric Mean Absolute Percentage Error (GMAPE)\n",
    "8. Percentage Root Mean Squared Deviation (PRMSD)\n",
    "9. Root Mean Squared Logarithmic Error (RMSLE)\n",
    "10. Weighted Absolute Percentage Error (WAPE)\n",
    "11. Absolute Squared Error (ASE)\n",
    "12. Normalized Mean Absolute Error (NMAE)\n",
    "13. Normalized Root Mean Squared Error (NRMSE)\n",
    "14. Mean Squared Scaled Error (MSSE)\n",
    "\n",
    "**Goodness-of-Fit Metrics:**\n",
    "\n",
    "15. R-squared (R²)\n",
    "16. Adjusted R-squared\n",
    "17. Theil's U statistic\n",
    "18. Akaike Information Criterion (AIC)\n",
    "19. Bayesian Information Criterion (BIC)\n",
    "20. F-statistic\n",
    "21. Log-Likelihood\n",
    "22. Residual Standard Error (RSE)\n",
    "23. Deviance\n",
    "24. Widely Applicable Information Criterion (WAIC)\n",
    "25. Log-Likelihood Ratio Test (LR Test)\n",
    "26. Bayesian p-value\n",
    "27. Bayesian Information Loss (BIL)\n",
    "\n",
    "**Residual Analysis Metrics:**\n",
    "\n",
    "28. Durbin-Watson Statistic\n",
    "29. Breusch-Pagan Test\n",
    "30. White's Test\n",
    "31. Jarque-Bera Test\n",
    "32. Ljung-Box Test\n",
    "33. Portmanteau Test\n",
    "34. Cumulative Sum of Residuals (CUSUM)\n",
    "\n",
    "**Outlier Detection Metrics:**\n",
    "\n",
    "35. Cook's Distance\n",
    "36. DFFITS\n",
    "37. Leverage\n",
    "38. Studentized Residuals\n",
    "39. Mahalanobis Distance\n",
    "40. Grubbs' Test\n",
    "41. Hampel Identifier\n",
    "\n",
    "**Heteroscedasticity Metrics:**\n",
    "\n",
    "42. Heteroscedasticity Test\n",
    "43. Goldfeld-Quandt Test\n",
    "44. Breusch-Pagan-Godfrey Test\n",
    "45. Park Test\n",
    "46. White's Test\n",
    "47. Bartlett's Test\n",
    "48. Brown-Forsythe Test\n",
    "49. O'Brien's Test\n",
    "\n",
    "**Autocorrelation Metrics:**\n",
    "\n",
    "50. Durbin-Watson Statistic\n",
    "51. Ljung-Box Test\n",
    "52. Portmanteau Test\n",
    "53. Autocorrelation Function (ACF)\n",
    "54. Partial Autocorrelation Function (PACF)\n",
    "\n",
    "**Correlation Metrics:**\n",
    "\n",
    "55. Pearson's Correlation Coefficient (r)\n",
    "56. Spearman's Rank-Order Correlation (ρ)\n",
    "57. Kendall's Tau\n",
    "58. Concordance Correlation Coefficient (CCC)\n",
    "59. Distance Correlation\n",
    "60. Phi Coefficient\n",
    "61. Cramer's V\n",
    "62. Point-Biserial Correlation\n",
    "63. Eta-squared\n",
    "\n",
    "**Multicollinearity Metrics:**\n",
    "\n",
    "64. Variance Inflation Factor (VIF)\n",
    "65. Condition Index\n",
    "66. Tolerance\n",
    "67. Collinearity Diagnostics\n",
    "\n",
    "**Robust Regression Metrics:**\n",
    "\n",
    "68. Huber Loss\n",
    "69. Tukey's Biweight Loss\n",
    "70. Least Trimmed Squares (LTS)\n",
    "71. RANSAC Loss\n",
    "72. M-Estimator Loss\n",
    "73. Breakdown Point\n",
    "\n",
    "**Stepwise Regression Metrics:**\n",
    "\n",
    "74. AIC and BIC\n",
    "75. Mallows' Cp\n",
    "76. Forward Selection\n",
    "77. Backward Elimination\n",
    "78. Recursive Feature Elimination (RFE)\n",
    "\n",
    "**Principal Component Regression Metrics:**\n",
    "\n",
    "79. Cumulative Variance Explained\n",
    "80. Principal Component Loadings\n",
    "81. Scree Plot\n",
    "\n",
    "**Quantile Regression Metrics:**\n",
    "\n",
    "82. Quantile Loss Function\n",
    "83. Quantile R-squared\n",
    "84. Quantile Regret\n",
    "\n",
    "**Time Series Metrics:**\n",
    "\n",
    "85. Mean Absolute Scaled Error (MASE)\n",
    "86. Forecast Bias\n",
    "87. Theil's U statistic\n",
    "88. Diebold-Mariano Test\n",
    "89. Ljung-Box Test for Residual Autocorrelation\n",
    "90. Autoregressive Integrated Moving Average (ARIMA) Metrics\n",
    "91. Seasonal Decomposition of Time Series (STL) Metrics\n",
    "\n",
    "**Spatial Regression Metrics:**\n",
    "\n",
    "92. Moran's I\n",
    "93. Geary's C\n",
    "94. Spatial Durbin Model Criterion\n",
    "95. Spatial Lag\n",
    "96. Spatial Autocorrelation (LISA)\n",
    "\n",
    "**Survival Analysis Metrics:**\n",
    "\n",
    "97. Concordance Index (C-index)\n",
    "98. Log-rank Test\n",
    "99. Brier Score\n",
    "100. Harrell's C\n",
    "101. Time-Dependent ROC (TD-ROC)\n",
    "102. Integrated Brier Score (IBS)\n",
    "\n",
    "**Model Complexity Metrics:**\n",
    "\n",
    "103. Complexity Penalty\n",
    "104. Number of Parameters\n",
    "105. Degrees of Freedom\n",
    "\n",
    "**Bayesian Regression Metrics:**\n",
    "\n",
    "106. Deviance Information Criterion (DIC)\n",
    "107. Widely Applicable Information Criterion (WAIC)\n",
    "108. Bayesian p-value\n",
    "109. Gelman-Rubin Convergence Diagnostic (R-hat)\n",
    "110. Posterior Predictive Checks (PPC)\n",
    "111. Credible Intervals (CI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeef342",
   "metadata": {},
   "source": [
    "# Error Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7a62c",
   "metadata": {},
   "source": [
    "Sure, let's go through the requested information for the error metrics you listed.\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "1. **What is the metric?**\n",
    "   - Mean Squared Error (MSE) is a metric that quantifies the average of the squared differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2$$\n",
    "   - Explanation: It calculates the squared differences between observed (Y) and predicted ($\\hat{Y}$) values, averages them, and provides a measure of the overall model fit.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - MSE is used to assess the accuracy of a regression model by quantifying the average squared differences between predicted and actual values.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - MSE penalizes larger errors more than smaller ones, making it suitable for models where outliers should be heavily penalized.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range is $[0, +∞)$, where lower values indicate better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for MSE. Lower values indicate better model fit, but what is considered \"low\" depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower MSE values indicate a better fit to the data. A model with an MSE of 0 perfectly predicts the target variable.\n",
    "\n",
    "8. **When to use:**\n",
    "   - MSE is widely used in regression problems when a quadratic loss function is appropriate. It is suitable for continuous target variables.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - MSE is sensitive to outliers. It may not be appropriate when outliers have a significant impact on model performance.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where outliers are a concern, you can consider using metrics like MAE or Huber loss, which are less sensitive to extreme values.\n",
    "\n",
    "11. **Code snippet:**\n",
    "    - You can use the `mean_squared_error` function from scikit-learn:\n",
    "    \n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "The remaining error metrics will be addressed in separate responses to maintain clarity and completeness for each metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3919a39",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Mean Absolute Error (MAE) is a metric that calculates the average of the absolute differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |Y_i - \\hat{Y}_i|$$\n",
    "   - Explanation: MAE measures the average absolute deviations between observed (Y) and predicted ($\\hat{Y}$) values.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - MAE is used to assess the accuracy of a regression model by quantifying the average absolute errors between predicted and actual values.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - MAE is less sensitive to outliers than MSE, making it suitable when outliers need to be considered but should not be heavily penalized.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range is $[0, +∞)$, with lower values indicating better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for MAE. Lower values indicate better model fit, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower MAE values indicate a better fit to the data. A model with an MAE of 0 perfectly predicts the target variable.\n",
    "\n",
    "8. **When to use:**\n",
    "   - MAE is suitable for regression problems when you want to measure the average absolute prediction error and when outliers should not have a heavy influence on the assessment.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - MAE may not be appropriate if you want to heavily penalize larger errors or if outliers have a significant impact on model performance.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - If you need to penalize larger errors more, MSE or a loss function like Huber loss may be more appropriate.\n",
    "\n",
    "11. **Code snippet:**\n",
    "    - You can use the `mean_absolute_error` function from scikit-learn:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "The information for the next error metric will be provided in the following response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc5dfe",
   "metadata": {},
   "source": [
    "Of course! Let's provide complete information for the Root Mean Squared Error (RMSE) and include a code snippet:\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - RMSE is a metric used to measure the accuracy of a regression model by calculating the square root of the average of the squared differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2}$$\n",
    "   - Explanation: RMSE is a more interpretable variant of MSE. It measures the average squared differences between observed (Y) and predicted ($\\hat{Y}$) values with the square root applied. This means RMSE provides results in the same unit as the target variable.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - RMSE is used to assess the accuracy of a regression model, and it is especially valuable when you want to express the error metric in the original units of the target variable.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - RMSE combines the benefits of MSE (penalizing larger errors more) with the advantage of providing a measure in the original units, making it highly interpretable.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of RMSE is $[0, +∞)$, where lower values indicate better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for RMSE. Lower RMSE values indicate better model fit, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower RMSE values indicate a better fit to the data, with a value of 0 representing a perfect prediction.\n",
    "\n",
    "8. **When to use:**\n",
    "   - RMSE is suitable for regression problems when you want a more interpretable error metric that is in the same units as the target variable.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - RMSE may not be appropriate if you have outliers in your data and do not want to heavily penalize them.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where outliers are a concern, Huber loss or other robust loss functions might be more appropriate.\n",
    "\n",
    "11. **Code snippet (Python with scikit-learn):**\n",
    "    - You can calculate RMSE using the square root of MSE as follows:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming y_true and y_pred are your actual and predicted values\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    ```\n",
    "\n",
    "This code calculates the RMSE given actual values (`y_true`) and predicted values (`y_pred`) using the mean squared error (MSE) as an intermediate step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20e153",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Mean Absolute Percentage Error (MAPE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Mean Absolute Percentage Error (MAPE) is a metric used to calculate the average percentage difference between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right| \\times 100\\%$$\n",
    "   - Explanation: MAPE quantifies the average absolute percentage errors between observed (Y) and predicted ($\\hat{Y}$) values. It is scaled to be a percentage by multiplying by 100%.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - MAPE is used to measure the percentage accuracy of a regression model, which can be more interpretable in some contexts, especially when working with non-dimensional data.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - MAPE provides a clear understanding of the average percentage error, making it easy to interpret and communicate model accuracy to non-technical stakeholders.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of MAPE is $[0\\%, +∞)$, where lower values indicate better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no universal threshold for MAPE, but lower values indicate better accuracy. The interpretation may vary depending on the problem domain.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A lower MAPE indicates a smaller percentage error between predicted and actual values. A MAPE of 0% means perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - MAPE is suitable when you want to express prediction errors as a percentage, making it helpful for non-technical audiences.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - MAPE may not be appropriate when dealing with data that contains zeros, as it can result in division by zero issues.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where zero values are present, alternatives like Mean Absolute Scaled Error (MASE) or geometric mean-based metrics might be more suitable.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate MAPE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate MAPE and then computes the MAPE given actual values (`y_true`) and predicted values (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6d292",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Median Absolute Percentage Error (MdAPE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Median Absolute Percentage Error (MdAPE) is a metric used to calculate the median of the absolute percentage differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: MdAPE is the median of the values $\\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right| \\times 100\\%$, where $i$ ranges from 1 to $n$.\n",
    "   - Explanation: MdAPE measures the median of the absolute percentage errors between observed (Y) and predicted ($\\hat{Y}$) values, expressed as a percentage.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - MdAPE is used to assess the median percentage accuracy of a regression model, focusing on the middle value in the distribution of errors.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - MdAPE is robust to outliers and provides a measure of central tendency for percentage errors, which can be more representative than mean-based metrics in the presence of extreme values.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of MdAPE is $[0\\%, +∞)$, with lower values indicating better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for MdAPE. Lower values are preferred, but the interpretation may vary depending on the problem domain.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A lower MdAPE indicates a smaller median percentage error between predicted and actual values. A MdAPE of 0% means perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - MdAPE is suitable when you want a robust measure of central tendency for percentage errors, particularly in situations with potential outliers.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - MdAPE may not be appropriate if you need to emphasize the influence of extreme values in your assessment.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where extreme values have a significant impact, you can consider using a metric like the Median Absolute Error (MdAE) or Median Absolute Deviation (MAD) of the errors.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate MdAPE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def median_absolute_percentage_error(y_true, y_pred):\n",
    "        absolute_percentage_errors = np.abs((y_true - y_pred) / y_true) * 100\n",
    "        return np.median(absolute_percentage_errors)\n",
    "\n",
    "    mdape = median_absolute_percentage_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate MdAPE and then computes the MdAPE given actual values (`y_true`) and predicted values (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43dd733",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next error metric.\n",
    "\n",
    "**Symmetric Mean Absolute Percentage Error (SMAPE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Symmetric Mean Absolute Percentage Error (SMAPE) is a metric used to calculate the average percentage difference between predicted and actual values while symmetrically accounting for the magnitude of the values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$SMAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|Y_i - \\hat{Y}_i|}{(|Y_i| + |\\hat{Y}_i|)/2} \\times 100\\%$$\n",
    "   - Explanation: SMAPE measures the average absolute percentage errors between observed (Y) and predicted ($\\hat{Y}$) values while symmetrically considering the magnitude of both the observed and predicted values.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - SMAPE is used to assess the percentage accuracy of a regression model, and it is symmetric, meaning it equally penalizes underestimation and overestimation.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - SMAPE provides a symmetric view of percentage errors and is suitable when both overestimation and underestimation should be treated similarly.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of SMAPE is $[0\\%, 200\\%]$, where lower values indicate better model performance. The upper bound of 200% represents a 100% underestimation and a 100% overestimation.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for SMAPE. Lower values are preferred, but the interpretation may vary depending on the problem domain.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower SMAPE values indicate a better fit to the data. A SMAPE of 0% means perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - SMAPE is suitable when you want to symmetrically evaluate percentage errors, which is common in forecasting and demand estimation.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - SMAPE may not be appropriate when you need to emphasize the impact of underestimation and overestimation differently.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where asymmetric treatment of underestimation and overestimation is required, consider using metrics like MAPE or other custom loss functions.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate SMAPE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "        absolute_percentage_errors = 200 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))\n",
    "        return np.mean(absolute_percentage_errors)\n",
    "\n",
    "    smape = symmetric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate SMAPE and then computes the SMAPE given actual values (`y_true`) and predicted values (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cc6688",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Geometric Mean Absolute Percentage Error (GMAPE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Geometric Mean Absolute Percentage Error (GMAPE) is a metric used to calculate the geometric mean of the absolute percentage differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$GMAPE = \\sqrt[n]{\\prod_{i=1}^{n} \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|} \\times 100\\%$$\n",
    "   - Explanation: GMAPE quantifies the geometric mean of the absolute percentage errors between observed (Y) and predicted ($\\hat{Y}$) values, expressed as a percentage.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - GMAPE is used to assess the central tendency of percentage accuracy of a regression model while incorporating all data points and is suitable for situations with varying magnitudes.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - GMAPE provides a balanced view of percentage errors across all data points and is robust to the impact of outliers.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of GMAPE is $[0\\%, +∞)$, where lower values indicate better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for GMAPE. Lower values are preferred, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower GMAPE values indicate a better fit to the data, with a GMAPE of 0% representing perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - GMAPE is suitable when you want a robust measure of central tendency for percentage errors across all data points, particularly in the presence of varying magnitudes.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - GMAPE may not be appropriate when you need to emphasize the impact of individual data points differently.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where the impact of individual data points should be treated differently, consider using custom loss functions or metrics like MAPE or MdAPE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate GMAPE using the following code:\n",
    "\n",
    "    ```python\n",
    "    from scipy.stats.mstats import gmean\n",
    "\n",
    "    def geometric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "        absolute_percentage_errors = np.abs((y_true - y_pred) / y_true) * 100\n",
    "        return gmean(absolute_percentage_errors)\n",
    "\n",
    "    gmape = geometric_mean_absolute_percentage_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate GMAPE and then computes the GMAPE given actual values (`y_true`) and predicted values (`y_pred`). The geometric mean is calculated using `gmean` from SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86387029",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Percentage Root Mean Squared Deviation (PRMSD):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Percentage Root Mean Squared Deviation (PRMSD) is a metric used to calculate the percentage version of the Root Mean Squared Error (RMSE), which measures the accuracy of a regression model while taking into account percentage differences.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$PRMSD = \\frac{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right)^2}}{\\sqrt{100}}$$\n",
    "   - Explanation: PRMSD is the percentage version of RMSE, where it calculates the square root of the average of the squared percentage differences between observed (Y) and predicted ($\\hat{Y}$) values.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - PRMSD is used to assess the percentage accuracy of a regression model in a way that is interpretable and directly comparable to RMSE.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - PRMSD provides a measure of accuracy while being directly interpretable as a percentage error, making it suitable for situations where a percentage error is more relevant.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of PRMSD is $[0, +∞)$, where lower values indicate better model performance. PRMSD is scaled to be interpretable as a percentage.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for PRMSD. Lower values are preferred, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower PRMSD values indicate a better fit to the data, with a PRMSD of 0% representing perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - PRMSD is suitable when you want to assess model accuracy in terms of a percentage error that is directly comparable to RMSE.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - PRMSD may not be appropriate when you need to emphasize the impact of underestimation and overestimation differently.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where asymmetric treatment of underestimation and overestimation is required, consider using metrics like RMSE or other custom loss functions.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate PRMSD using the following code:\n",
    "\n",
    "    ```python\n",
    "    def percentage_root_mean_squared_deviation(y_true, y_pred):\n",
    "        squared_percentage_errors = ((y_true - y_pred) / y_true) ** 2\n",
    "        prmsd = np.sqrt(np.mean(squared_percentage_errors)) * 100\n",
    "        return prmsd\n",
    "    prmsd = percentage_root_mean_squared_deviation(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate PRMSD and then computes the PRMSD given actual values (`y_true`) and predicted values (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a703cda0",
   "metadata": {},
   "source": [
    "Of course, let's continue with the information for the next error metric.\n",
    "\n",
    "**Root Mean Squared Logarithmic Error (RMSLE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Root Mean Squared Logarithmic Error (RMSLE) is a metric used to calculate the square root of the average of the squared logarithmic differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$RMSLE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\log(Y_i + 1) - \\log(\\hat{Y}_i + 1))^2}$$\n",
    "   - Explanation: RMSLE measures the square root of the average squared logarithmic differences between observed (Y) and predicted ($\\hat{Y}$) values. The addition of 1 in the logarithmic terms avoids issues with zero values.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - RMSLE is used to assess the accuracy of a regression model while penalizing large relative errors, making it suitable for data with a wide range of values.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - RMSLE is particularly useful when the target variable has a large range, and you want to penalize underestimation and overestimation symmetrically on a logarithmic scale.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of RMSLE is $[0, +∞)$, where lower values indicate better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for RMSLE. Lower values are preferred, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower RMSLE values indicate a better fit to the data, with a value of 0 representing perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - RMSLE is suitable when you want to penalize errors on a logarithmic scale and when the target variable has a large range.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - RMSLE may not be appropriate when you need to emphasize the impact of relative errors differently.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where relative errors should be treated asymmetrically or when the target variable does not have a wide range, consider using metrics like RMSE or MAE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate RMSLE using the following code:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_log_error\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming y_true and y_pred are your actual and predicted values\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_true + 1, y_pred + 1))\n",
    "    ```\n",
    "\n",
    "This code uses the `mean_squared_log_error` function from scikit-learn to calculate RMSLE while adding 1 to both actual and predicted values to avoid issues with zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eef70a2",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Weighted Absolute Percentage Error (WAPE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Weighted Absolute Percentage Error (WAPE) is a metric used to calculate the weighted average of the absolute percentage differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$WAPE = \\frac{\\sum_{i=1}^{n} w_i \\cdot \\left|\\frac{Y_i - \\hat{Y}_i}{Y_i}\\right|}{\\sum_{i=1}^{n} w_i} \\times 100\\%$$\n",
    "   - Explanation: WAPE computes the weighted average of absolute percentage errors between observed (Y) and predicted ($\\hat{Y}$) values. It takes into account the weights ($w_i$) assigned to each data point.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - WAPE is used to assess the weighted percentage accuracy of a regression model, allowing for different levels of importance for individual data points.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - WAPE is valuable when you need to emphasize the accuracy of specific data points more than others by assigning appropriate weights.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of WAPE is $[0\\%, +∞)$, with lower values indicating better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no universal threshold for WAPE. The interpretation of ideal values depends on the problem and the assigned weights.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower WAPE values indicate a better fit to the data, with a value of 0% representing perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - WAPE is suitable when you have domain knowledge that allows you to assign different weights to data points based on their relative importance.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - WAPE may not be appropriate when you cannot assign meaningful weights to data points.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where assigning weights is not feasible, consider using standard metrics like MAPE or RMSE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate WAPE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def weighted_absolute_percentage_error(y_true, y_pred, weights):\n",
    "        absolute_percentage_errors = np.abs((y_true - y_pred) / y_true) * 100\n",
    "        wape = np.sum(weights * absolute_percentage_errors) / np.sum(weights)\n",
    "        return wape\n",
    "\n",
    "    # Assuming weights is a list or array of weights for each data point\n",
    "    wape = weighted_absolute_percentage_error(y_true, y_pred, weights)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate WAPE, taking into account the weights assigned to each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5b0a3",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next error metric.\n",
    "\n",
    "**Absolute Squared Error (ASE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Absolute Squared Error (ASE) is a metric used to calculate the sum of the squared absolute differences between predicted and actual values.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$ASE = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2$$\n",
    "   - Explanation: ASE measures the sum of the squared absolute errors between observed (Y) and predicted ($\\hat{Y}$) values, without considering the sign of the errors.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - ASE is used to quantify the overall squared error in a regression model, which can be useful when the sign of the errors is not relevant.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - ASE provides a measure of overall squared error and is useful when the direction of the errors (underestimation or overestimation) is less important.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of ASE is $[0, +∞)$, with lower values indicating better model performance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for ASE. Lower values are preferred, but the interpretation depends on the problem context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower ASE values indicate a better fit to the data, with an ASE of 0 representing perfect predictions.\n",
    "\n",
    "8. **When to use:**\n",
    "   - ASE is suitable when you want to assess the overall squared error without considering the direction of the errors.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - ASE may not be appropriate when you need to consider the sign of the errors or when relative errors are more important.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where considering the direction of errors is important, consider metrics like RMSE or MAE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate ASE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def absolute_squared_error(y_true, y_pred):\n",
    "        squared_errors = (y_true - y_pred) ** 2\n",
    "        ase = np.sum(squared_errors)\n",
    "        return ase\n",
    "\n",
    "    ase = absolute_squared_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate ASE based on the sum of squared absolute errors between actual values (`y_true`) and predicted values (`y_pred`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb1745",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Normalized Mean Absolute Error (NMAE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Normalized Mean Absolute Error (NMAE) is a metric used to calculate the mean absolute error (MAE) normalized by the range of the target variable.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$NMAE = \\frac{\\text{MAE}}{\\max(Y) - \\min(Y)}$$\n",
    "   - Explanation: NMAE calculates the MAE and then normalizes it by the range (maximum - minimum) of the target variable. This normalization makes the metric scale-independent.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - NMAE is used to assess the average absolute prediction error relative to the scale of the target variable, making it useful for comparing models across different data sets.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - NMAE provides a standardized measure of error that is scale-independent, allowing for comparisons between models applied to data with different units or ranges.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of NMAE is $[0, 1]$, where lower values indicate better model performance. A value of 0 represents a perfect prediction.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for NMAE is 0, indicating perfect predictions. Higher values indicate larger errors.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A lower NMAE indicates a better fit to the data. NMAE values can be compared across models regardless of the data's scale.\n",
    "\n",
    "8. **When to use:**\n",
    "   - NMAE is suitable when you want to compare model performance on different data sets or when you need a scale-independent error metric.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - NMAE may not be appropriate when you want to emphasize the impact of errors on the original scale of the data.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where the scale of the target variable is important, consider using MAE or RMSE, which provide absolute error measures.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate NMAE using the following code:\n",
    "\n",
    "    ```python\n",
    "    def normalized_mean_absolute_error(y_true, y_pred):\n",
    "        mae = np.mean(np.abs(y_true - y_pred))\n",
    "        range_of_target = np.max(y_true) - np.min(y_true)\n",
    "        nmae = mae / range_of_target\n",
    "        return nmae\n",
    "\n",
    "    nmae = normalized_mean_absolute_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate NMAE by first calculating MAE and then normalizing it by the range of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66d92df",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next error metric.\n",
    "\n",
    "**Normalized Root Mean Squared Error (NRMSE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Normalized Root Mean Squared Error (NRMSE) is a metric used to calculate the root mean squared error (RMSE) normalized by the range of the target variable.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$NRMSE = \\frac{\\sqrt{\\text{RMSE}^2}}{\\max(Y) - \\min(Y)}$$\n",
    "   - Explanation: NRMSE calculates the RMSE and then normalizes it by the range (maximum - minimum) of the target variable. This normalization makes the metric scale-independent.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - NRMSE is used to assess the RMSE in a way that is normalized to the scale of the target variable, making it useful for comparing models on different datasets or scales.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - NRMSE provides a standardized measure of error that is scale-independent, allowing for comparisons between models applied to data with different units or ranges.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of NRMSE is $[0, 1]$, where lower values indicate better model performance. A value of 0 represents a perfect prediction.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for NRMSE is 0, indicating perfect predictions. Higher values indicate larger errors.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A lower NRMSE indicates a better fit to the data. NRMSE values can be compared across models regardless of the data's scale.\n",
    "\n",
    "8. **When to use:**\n",
    "   - NRMSE is suitable when you want to compare model performance on different data sets or when you need a scale-independent error metric.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - NRMSE may not be appropriate when you want to emphasize the impact of errors on the original scale of the data.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where the scale of the target variable is important, consider using RMSE, which provides an absolute error measure.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate NRMSE using the following code:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    def normalized_root_mean_squared_error(y_true, y_pred):\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        range_of_target = np.max(y_true) - np.min(y_true)\n",
    "        nrmse = rmse / range_of_target\n",
    "        return nrmse\n",
    "\n",
    "    nrmse = normalized_root_mean_squared_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code uses scikit-learn to calculate RMSE and then normalizes it by the range of the target variable to obtain NRMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb095bf2",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next error metric.\n",
    "\n",
    "**Mean Squared Scaled Error (MSSE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Mean Squared Scaled Error (MSSE) is a metric used to calculate the mean squared error (MSE) scaled by the variance of the target variable.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: $$MSSE = \\frac{\\text{MSE}}{\\text{Var}(Y)}$$\n",
    "   - Explanation: MSSE computes the MSE and then scales it by the variance (Var) of the target variable (Y). This scaling allows for a relative assessment of the error.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - MSSE is used to measure the MSE in a way that accounts for the variability in the target variable, making it suitable for data with varying spread.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - MSSE provides a measure of error that is relative to the variance of the target variable, allowing for comparisons across datasets with different variability.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of MSSE is $[0, +∞)$, with lower values indicating better model performance. A value of 0 represents a perfect prediction.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for MSSE is 0, indicating perfect predictions. Higher values indicate larger errors relative to the variance.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A lower MSSE indicates a better fit to the data, considering the variability in the target variable.\n",
    "\n",
    "8. **When to use:**\n",
    "   - MSSE is suitable when you want to assess model error in a way that is relative to the variance of the target variable.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - MSSE may not be appropriate when the relative error to the variance is not of interest, or when variance is very close to zero.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For situations where assessing error in an absolute sense is more relevant, consider using MSE or RMSE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate MSSE using the following code:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    def mean_squared_scaled_error(y_true, y_pred):\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        variance_of_target = np.var(y_true)\n",
    "        msse = mse / variance_of_target\n",
    "        return msse\n",
    "\n",
    "    msse = mean_squared_scaled_error(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code uses scikit-learn to calculate MSE and then scales it by the variance of the target variable to obtain MSSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96426385",
   "metadata": {},
   "source": [
    "# Goodness-of-Fit Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a3d76",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**R-squared (R²):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - R-squared (R²), also known as the coefficient of determination, is a statistical metric that measures the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[R^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\\]\n",
    "   - Explanation: R² quantifies the goodness of fit of a regression model. It compares the sum of squared errors (SSE), which represents the unexplained variance, to the total sum of squares (SST), which represents the total variance in the dependent variable.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - R² is used to assess how well a regression model explains the variability in the dependent variable. Higher R² values indicate a better fit.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - R² provides a measure of the goodness of fit and helps understand the percentage of variance explained by the model.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - R² ranges from 0 to 1, where 0 indicates that the model explains none of the variance, and 1 indicates that the model explains all the variance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - A higher R² is generally preferred, but the ideal threshold depends on the context and the specific problem.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - An R² of 1 indicates a perfect fit, while an R² of 0 means the model doesn't explain any variance. Values between 0 and 1 represent the proportion of explained variance.\n",
    "\n",
    "8. **When to use:**\n",
    "   - R² is suitable for assessing the overall goodness of fit of a regression model and comparing different models.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - R² may not be appropriate when dealing with models that have multicollinearity or when you want to assess the predictive power of a model rather than the explanatory power.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For predictive modeling, metrics like RMSE or MAE may be more appropriate.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate R² using scikit-learn as follows:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    r_squared = r2_score(y_true, y_pred)\n",
    "    ```\n",
    "\n",
    "This code uses the `r2_score` function from scikit-learn to compute the R² of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b14607a",
   "metadata": {},
   "source": [
    "Certainly, let's continue with the information for the next metric.\n",
    "\n",
    "**Adjusted R-squared:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Adjusted R-squared is a modification of the R-squared (R²) metric that adjusts for the number of predictors in a regression model. It provides a measure of the proportion of variance explained while penalizing the inclusion of unnecessary predictors.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[Adjusted \\, R^2 = 1 - \\frac{\\frac{SSE}{n - p - 1}}{\\frac{SST}{n - 1}}\\]\n",
    "   - Explanation: Adjusted R-squared adjusts the R-squared value by considering both the explained variance (SSE) and the degrees of freedom associated with the number of predictors (p) and the sample size (n).\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - Adjusted R-squared is used to assess the goodness of fit of a regression model while accounting for the impact of adding predictors. It helps identify if adding more predictors improves the model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - Adjusted R-squared provides a more realistic estimate of the model's performance by adjusting for the risk of overfitting due to excessive predictor variables.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - Adjusted R-squared also ranges from 0 to 1, with 0 indicating that the model explains none of the variance and 1 indicating that the model perfectly explains the variance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - Higher adjusted R-squared values are generally preferred. However, the ideal threshold varies depending on the specific problem.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - An adjusted R-squared value close to 1 suggests that the model effectively explains the variance, while a value close to 0 indicates that the model does not explain much variance, considering the number of predictors.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Adjusted R-squared is appropriate when you want to assess model fit while considering the impact of predictor variables and avoiding overfitting.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - Adjusted R-squared may not be suitable when assessing predictive power, as it primarily measures the explanatory power of the model.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For predictive modeling, consider metrics like RMSE or MAE, which focus on prediction accuracy.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate Adjusted R-squared using Python as follows:\n",
    "\n",
    "    ```python\n",
    "    def adjusted_r_squared(y_true, y_pred, n, p):\n",
    "        r_squared = r2_score(y_true, y_pred)\n",
    "        adjusted_r2 = 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "        return adjusted_r2\n",
    "\n",
    "    adjusted_r2 = adjusted_r_squared(y_true, y_pred, n, p)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate the Adjusted R-squared based on the R² value, sample size (n), and the number of predictors (p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8686f6be",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Theil's U Statistic:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Theil's U Statistic, often referred to as Theil's U or U Statistic, is a statistical metric used to assess the inequality or dissimilarity between two time series or datasets.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[U = \\frac{\\sqrt{\\sum_{t=2}^{T} \\left(\\frac{y_t - y_{t-1}}{y_t}\\right)^2}}{T-1}\\]\n",
    "   - Explanation: Theil's U Statistic calculates the average proportional change between consecutive data points in a time series. It quantifies the degree of change or dissimilarity between the data points.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - Theil's U Statistic is used to measure the relative change or dissimilarity between data points over time, making it suitable for analyzing trends, patterns, or inequality in time series data.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - Theil's U provides a quantifiable measure of change, making it valuable for understanding the dynamics and trends within a time series.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of Theil's U Statistic is \\([0, +∞)\\), where higher values indicate greater relative change or inequality.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for Theil's U Statistic. The interpretation depends on the specific problem and context.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Higher values of Theil's U Statistic suggest greater relative change or dissimilarity between data points.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Theil's U Statistic is suitable when you want to assess relative change, inequality, or dissimilarity between time series data points.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - Theil's U Statistic may not be appropriate when assessing the absolute magnitude of changes or when comparing data with different scales.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - If you need to measure the absolute magnitude of changes in time series data, consider using other metrics like RMSE or MAE.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate Theil's U Statistic using the following code:\n",
    "\n",
    "    ```python\n",
    "    def theils_u(y):\n",
    "        T = len(y)\n",
    "        u = 0\n",
    "        for t in range(1, T):\n",
    "            u += (y[t] - y[t - 1]) ** 2 / y[t]\n",
    "        u = (u / (T - 1)) ** 0.5\n",
    "        return u\n",
    "\n",
    "    theils_u_statistic = theils_u(y)\n",
    "    ```\n",
    "\n",
    "This code defines a custom function to calculate Theil's U Statistic based on the given time series data `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b6eab",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Akaike Information Criterion (AIC):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Akaike Information Criterion (AIC) is a metric used for model selection in the context of statistical modeling and regression. It provides a trade-off between the goodness of fit of the model and the complexity of the model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[AIC = 2k - 2\\ln(L)\\]\n",
    "   - Explanation: AIC combines two components. The first term, \\(2k\\), represents a penalty for the number of model parameters (predictors), where \\(k\\) is the number of parameters. The second term, \\(-2\\ln(L)\\), is related to the likelihood of the model, where \\(L\\) is the likelihood function of the model. AIC aims to minimize this trade-off.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - AIC is used to compare models by balancing goodness of fit and model complexity. It helps in selecting the most parsimonious model that explains the data well.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - AIC provides a quantitative way to assess and compare the appropriateness of different models, making it valuable for model selection and simplification.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - AIC has no fixed range. Lower AIC values indicate better-fitting and more parsimonious models.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for AIC is the lowest possible value among the models being compared. A lower AIC suggests a better trade-off between fit and complexity.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Comparing AIC values across models allows you to select the model with the lowest AIC, which indicates the best trade-off between model fit and complexity.\n",
    "\n",
    "8. **When to use:**\n",
    "   - AIC is suitable for model selection and comparison in the context of regression analysis and other statistical modeling tasks.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - AIC may not be appropriate in cases where a simpler model is preferred without a specific need for model comparison.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - AIC is particularly well-suited for model selection, but other model selection criteria like the Bayesian Information Criterion (BIC) can be used as alternatives.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate AIC using the following code with statsmodels:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    aic = sm.OLS(model.endog, model.exog).fit().aic\n",
    "    ```\n",
    "\n",
    "This code calculates the AIC for a regression model using the `OLS` function from the statsmodels library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b362a",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Bayesian Information Criterion (BIC):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Bayesian Information Criterion (BIC) is a metric used for model selection in the context of statistical modeling and regression. Similar to AIC, it provides a trade-off between model goodness of fit and complexity.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[BIC = -2\\ln(L) + k\\ln(n)\\]\n",
    "   - Explanation: BIC combines two components. The first term, \\(-2\\ln(L)\\), is related to the likelihood of the model, where \\(L\\) is the likelihood function of the model. The second term, \\(k\\ln(n)\\), represents a penalty for the number of model parameters (predictors) where \\(k\\) is the number of parameters, and \\(n\\) is the sample size.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - BIC is used to compare models by striking a balance between goodness of fit and model complexity. It helps in selecting the most parsimonious model that explains the data well.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - BIC provides a quantitative way to assess and compare the appropriateness of different models, making it valuable for model selection and simplification.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - BIC has no fixed range. Lower BIC values indicate better-fitting and more parsimonious models.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for BIC is the lowest possible value among the models being compared. A lower BIC suggests a better trade-off between fit and complexity.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Comparing BIC values across models allows you to select the model with the lowest BIC, indicating the best trade-off between model fit and complexity.\n",
    "\n",
    "8. **When to use:**\n",
    "   - BIC is suitable for model selection and comparison in the context of regression analysis and other statistical modeling tasks.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - BIC may not be appropriate in cases where a simpler model is preferred without a specific need for model comparison.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - BIC is a strong alternative for model selection, but it can be complemented by other criteria such as AIC when appropriate.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate BIC using the following code with statsmodels:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    bic = sm.OLS(model.endog, model.exog).fit().bic\n",
    "    ```\n",
    "\n",
    "This code calculates the BIC for a regression model using the `OLS` function from the statsmodels library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5088e6f",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**F-statistic:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The F-statistic is a statistical metric used to test the overall significance of a regression model. It assesses whether at least one of the independent variables significantly contributes to explaining the variation in the dependent variable.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[F = \\frac{\\text{Explained Variance}/\\text{Number of Predictors}}{\\text{Unexplained Variance}/\\text{Degrees of Freedom}}\\]\n",
    "   - Explanation: The F-statistic calculates the ratio of explained variance (variance explained by the model) to unexplained variance (residual variance) and adjusts for the number of predictors and degrees of freedom.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The F-statistic is used to determine whether a regression model, as a whole, is statistically significant. It helps in assessing the overall fit of the model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - The F-statistic provides a statistical test to decide if the regression model adds value in explaining the variance in the dependent variable.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The F-statistic follows an F-distribution, and its range depends on the degrees of freedom. Higher F-values indicate greater significance.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for the F-statistic depends on the chosen significance level (alpha). Typically, a lower p-value associated with the F-statistic suggests statistical significance.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - If the p-value associated with the F-statistic is less than the chosen significance level (alpha), the model is considered statistically significant, indicating that at least one predictor has a significant effect on the dependent variable.\n",
    "\n",
    "8. **When to use:**\n",
    "   - The F-statistic is suitable for testing the overall significance of a regression model and assessing if it explains a significant portion of the variance.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - The F-statistic is not typically used as an evaluation metric but rather as a test for model significance. It may not be useful for assessing individual predictors or model fit.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing individual predictors or model fit, use metrics like t-statistics for predictors, R-squared, or adjusted R-squared.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can obtain the F-statistic and associated p-value from a regression model in Python using libraries like statsmodels:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    f_statistic = model.fvalue\n",
    "    p_value = model.f_pvalue\n",
    "    ```\n",
    "\n",
    "This code retrieves the F-statistic and its associated p-value from a regression model using the statsmodels library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f587be",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Log-Likelihood:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Log-Likelihood is a statistical metric used in the context of maximum likelihood estimation. It quantifies the goodness of fit of a statistical model to the observed data.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The log-likelihood formula varies depending on the specific statistical model used, but in general, it represents the logarithm of the likelihood function, which is a measure of how well the model explains the observed data.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - Log-Likelihood is used to assess how well a statistical model fits the observed data. It is particularly important in maximum likelihood estimation, where the goal is to maximize this function.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - Log-Likelihood provides a quantitative measure of the goodness of fit, allowing for comparisons between different models.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of log-likelihood values depends on the specific statistical model and data. Higher log-likelihood values indicate a better fit.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for log-likelihood. It is typically used in the context of model comparison, where the model with a higher log-likelihood is preferred.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Log-Likelihood values can be compared between different models. Models with higher log-likelihoods are considered to provide a better fit to the data.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Log-Likelihood is suitable when assessing the goodness of fit of statistical models and when comparing the fit of different models.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - Log-Likelihood may not be appropriate when assessing models with very different complexities or when specific model fit measures are needed.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - Log-Likelihood is well-suited for model comparison, but other model-specific fit measures or information criteria (e.g., AIC, BIC) can be used for specific purposes.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating log-likelihood can be specific to the statistical model being used. For example, in a linear regression model using statsmodels, you can obtain the log-likelihood as follows:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your linear regression model\n",
    "    log_likelihood = model.llf\n",
    "    ```\n",
    "\n",
    "This code retrieves the log-likelihood from a linear regression model using the statsmodels library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f09334",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Residual Standard Error (RSE):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Residual Standard Error (RSE), also known as the Residual Sum of Squares (RSS), is a statistical metric used in the context of linear regression. It quantifies the variability of the observed data points around the regression line.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Formula: \\[RSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n - p - 1}}\\]\n",
    "   - Explanation: RSE calculates the standard deviation of the residuals, which are the differences between the observed values (\\(y_i\\)) and the predicted values (\\(\\hat{y}_i\\)) from a linear regression model. The square of each residual is summed up, divided by the degrees of freedom (\\(n - p - 1\\)), and then the square root is taken to obtain the standard error of the residuals.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - RSE is used to estimate the standard deviation of the errors (residuals) in a linear regression model. It measures the variability of the data points around the regression line.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - RSE provides a measure of how well the model fits the data by quantifying the typical size of the residuals. It is a useful metric for assessing model performance and prediction accuracy.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of RSE depends on the specific data and model but is typically expressed in the same units as the dependent variable.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for RSE. A lower RSE value indicates a better fit and smaller errors.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Smaller RSE values indicate that the model provides a better fit to the data, with smaller residuals and less variability around the regression line.\n",
    "\n",
    "8. **When to use:**\n",
    "   - RSE is suitable for assessing the precision and accuracy of predictions in a linear regression model.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - RSE may not be suitable when assessing models with different functional forms or when dealing with non-linear regression.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For non-linear regression, consider using metrics specific to the model, such as the coefficient of determination (R-squared).\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate RSE for a linear regression model using the following code with scikit-learn:\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import numpy as np\n",
    "\n",
    "    def residual_standard_error(y_true, y_pred, p):\n",
    "        n = len(y_true)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rse = np.sqrt(mse / (n - p - 1))\n",
    "        return rse\n",
    "\n",
    "    rse = residual_standard_error(y_true, y_pred, p)\n",
    "    ```\n",
    "\n",
    "This code computes the RSE for a linear regression model by first calculating the mean squared error (MSE) and then scaling it by the degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c41b29",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Deviance:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Deviance is a statistical metric used in the context of generalized linear models (GLMs) to assess the goodness of fit of the model to the observed data.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - Deviance is a complex concept and has different formulations depending on the specific GLM used. In the context of Poisson regression, for example, it can be calculated as: \\[Deviance = 2 \\sum_{i=1}^{n} \\left(y_i \\cdot \\ln\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i)\\right)\\]\n",
    "   - Explanation: Deviance measures the difference between the likelihood of the model compared to a saturated model, which perfectly fits the observed data. It assesses how well the model explains the data compared to the best possible fit.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - Deviance is used to determine how well a GLM fits the data and assesses the appropriateness of the model's assumptions.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - Deviance provides a measure of the goodness of fit for GLMs, making it valuable for model selection and evaluation.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of deviance depends on the specific GLM and data. Smaller deviance values indicate a better fit.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for deviance. Lower deviance values are preferred, indicating a better fit to the data.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Smaller deviance values suggest that the model provides a better fit to the data compared to a saturated model.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Deviance is suitable for assessing the goodness of fit of generalized linear models, especially when dealing with count data or other non-normally distributed data.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - Deviance is not typically used for assessing models that are not based on the GLM framework.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - When working with linear regression, metrics like R-squared or residual standard error (RSE) may be more appropriate.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating deviance depends on the specific GLM and its implementation in Python. Here's an example of how to calculate deviance for a Poisson regression model using the `statsmodels` library:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your Poisson regression model\n",
    "    deviance = model.deviance\n",
    "    ```\n",
    "\n",
    "This code retrieves the deviance from a Poisson regression model using the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227afa7",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Widely Applicable Information Criterion (WAIC):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Widely Applicable Information Criterion (WAIC) is a metric used for model comparison and selection, especially in Bayesian statistics. It assesses the fit and predictive accuracy of Bayesian models.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - WAIC is computed using a complex formula based on the log-likelihood of the model and the effective number of parameters. The formula is detailed and varies based on the specific Bayesian model.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - WAIC is used to compare different Bayesian models and select the one that provides the best balance between fit and complexity. It is particularly suited for models with complex structures.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - WAIC provides a measure of a model's predictive accuracy and goodness of fit in a Bayesian framework, making it valuable for Bayesian model selection.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of WAIC values depends on the specific Bayesian model and data. Lower WAIC values indicate better model fit and predictive accuracy.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific threshold for WAIC. Lower WAIC values are preferred as they indicate better model fit and predictive performance.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Comparing WAIC values across different Bayesian models allows you to select the model with the lowest WAIC, which suggests better predictive accuracy.\n",
    "\n",
    "8. **When to use:**\n",
    "   - WAIC is suitable when comparing and selecting Bayesian models, especially in situations where complex models need to be evaluated.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - WAIC may not be appropriate when dealing with non-Bayesian models or when simpler models are sufficient for the task.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For non-Bayesian models, use alternative criteria such as AIC or BIC for model comparison.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating WAIC in Python requires specialized libraries for Bayesian modeling, such as `pymc3`. Below is an example of how to calculate WAIC for a Bayesian model:\n",
    "\n",
    "    ```python\n",
    "    import pymc3 as pm\n",
    "\n",
    "    # Assuming model is your Bayesian model\n",
    "    waic = pm.waic(model)\n",
    "    ```\n",
    "\n",
    "This code calculates WAIC for a Bayesian model using the `pymc3` library. The actual implementation may vary based on the specific Bayesian modeling framework used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e56c0",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Log-Likelihood Ratio Test (LR Test):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Log-Likelihood Ratio Test (LR Test) is a statistical metric used for model comparison. It assesses whether one model provides a significantly better fit to the data compared to another model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The LR Test calculates the difference in the log-likelihoods of two models: one nested within the other. The test statistic follows a chi-squared distribution.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The LR Test is used to compare the fit of two nested models and determine if the more complex model significantly improves the fit compared to the simpler model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - LR Test provides a formal statistical framework for comparing models and helps in selecting the most appropriate model.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution, and the range depends on the specific data and models being compared.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for the LR Test is based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, the more complex model is considered significantly better.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) suggests that the more complex model provides a significantly better fit to the data than the simpler model.\n",
    "\n",
    "8. **When to use:**\n",
    "   - LR Test is suitable when comparing nested models, especially in regression and statistical modeling.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - LR Test is not suitable for comparing non-nested models or assessing the overall goodness of fit of a single model.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For comparing non-nested models, metrics like AIC or BIC can be used. For assessing the overall goodness of fit, use metrics like R-squared.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating the LR Test in Python requires specialized libraries for statistical modeling, such as `statsmodels`. Below is an example of how to perform an LR Test to compare two nested models:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model1 and model2 are your two nested models\n",
    "    lr_test = model1.compare_lr_test(model2)\n",
    "    ```\n",
    "\n",
    "This code performs an LR Test to compare two nested models using the `compare_lr_test` method in the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb414cf",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Bayesian p-value:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Bayesian p-value is a statistical metric used in Bayesian hypothesis testing. It assesses the probability that a given hypothesis is true based on observed data and a Bayesian model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The Bayesian p-value is obtained by calculating the probability of observing data as extreme as, or more extreme than, the observed data under the given hypothesis. It is computed based on the posterior distribution of the model.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Bayesian p-value is used to assess the strength of evidence for or against a hypothesis in a Bayesian framework. It provides a more flexible and interpretable way to perform hypothesis tests.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - The Bayesian p-value allows for a more probabilistic interpretation of hypothesis testing, considering the uncertainty in parameter estimates and model assumptions.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of Bayesian p-values is from 0 to 1, where values close to 0 suggest strong evidence against the hypothesis, and values close to 1 suggest strong support for the hypothesis.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold for a Bayesian p-value depends on the chosen significance level and the specific problem. Commonly used significance levels are 0.05 and 0.01.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small Bayesian p-value (typically less than the chosen significance level) suggests evidence against the hypothesis, while a large Bayesian p-value supports the hypothesis.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Bayesian p-values are suitable for hypothesis testing within a Bayesian framework, especially when dealing with complex models and parameter uncertainty.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - Bayesian p-values may not be suitable when performing frequentist hypothesis tests, as they are designed for Bayesian analysis.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - In a frequentist framework, traditional p-values are commonly used for hypothesis testing. For model selection, Bayesian Information Criterion (BIC) or Akaike Information Criterion (AIC) can be alternatives.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating Bayesian p-values requires specialized libraries for Bayesian analysis, such as `pymc3`. Below is an example of how to compute a Bayesian p-value for a hypothesis using `pymc3`:\n",
    "\n",
    "    ```python\n",
    "    import pymc3 as pm\n",
    "\n",
    "    # Assuming observed_data is your observed data and model is your Bayesian model\n",
    "    with model:\n",
    "        trace = pm.sample(1000)\n",
    "        posterior = pm.sample_posterior_predictive(trace)\n",
    "        observed_data = ...  # Your observed data\n",
    "        p_value = (posterior['observed_data'] > observed_data).mean()\n",
    "    ```\n",
    "\n",
    "This code demonstrates how to calculate a Bayesian p-value by sampling from the posterior distribution and comparing the observed data to samples from the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86315ce3",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next metric.\n",
    "\n",
    "**Bayesian Information Loss (BIL):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - Bayesian Information Loss (BIL) is a statistical metric used to assess the information loss incurred when using one model to approximate another, typically in a Bayesian framework.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The formula for BIL can vary based on the specific context. In general, it quantifies the difference in information content between the approximating model and the true model.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - BIL is used to evaluate the quality of an approximation in Bayesian analysis. It helps determine how well one model represents another and assesses the loss of information.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - BIL provides a quantitative measure of the quality of an approximation, which is valuable in situations where simplifying complex models is necessary.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The range of BIL values depends on the specific context and models being compared.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - There is no specific ideal threshold for BIL. The interpretation of BIL values depends on the specific problem and the trade-off between approximation quality and computational simplicity.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - Lower BIL values suggest a better approximation and less information loss, while higher BIL values indicate more significant information loss.\n",
    "\n",
    "8. **When to use:**\n",
    "   - BIL is suitable when approximating complex models with simpler models in a Bayesian framework, often in the context of computational efficiency.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - BIL is not typically used for model evaluation in non-Bayesian settings or when dealing with non-Bayesian approximations.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - When dealing with non-Bayesian models or when evaluating model fit, alternative criteria like AIC, BIC, or cross-validation can be used.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating BIL depends on the specific context and Bayesian models being used. It involves comparing the information content of different models, and the implementation can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c11738",
   "metadata": {},
   "source": [
    "# Residual Analysis Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c672dd3",
   "metadata": {},
   "source": [
    "Let's start with the information for the first residual analysis metric:\n",
    "\n",
    "**Durbin-Watson Statistic:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Durbin-Watson Statistic is a statistical metric used to detect the presence of autocorrelation in the residuals of a regression model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The Durbin-Watson Statistic is calculated as: \\[DW = \\frac{\\sum_{i=2}^{n}(e_i - e_{i-1})^2}{\\sum_{i=1}^{n}e_i^2}\\]\n",
    "   - Explanation: It measures the degree of autocorrelation by comparing the squared differences between consecutive residuals to the squared residuals themselves.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Durbin-Watson Statistic is used to assess whether there is a significant autocorrelation in the residuals, which is essential for the validity of regression assumptions.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps identify and address autocorrelation issues in a regression model, ensuring the reliability of parameter estimates.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The Durbin-Watson Statistic ranges from 0 to 4. A value around 2 indicates no significant autocorrelation, while values significantly different from 2 indicate the presence of autocorrelation.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is around 2. Values close to 2 suggest no significant autocorrelation, while values significantly below or above 2 indicate autocorrelation.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A Durbin-Watson Statistic close to 2 suggests no significant autocorrelation. Values below 2 indicate positive autocorrelation, while values above 2 indicate negative autocorrelation.\n",
    "\n",
    "8. **When to use:**\n",
    "   - Durbin-Watson is suitable for assessing autocorrelation in time series or regression models where autocorrelation is a concern.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other types of regression model assumptions or for models where autocorrelation is not a concern.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing other regression assumptions, use metrics like heteroscedasticity tests or normality tests.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can calculate the Durbin-Watson Statistic in Python using libraries like statsmodels:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    dw_statistic = sm.stats.stattools.durbin_watson(model.resid)\n",
    "    ```\n",
    "\n",
    "This code calculates the Durbin-Watson Statistic for the residuals of a regression model using the `durbin_watson` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655b06c",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**Breusch-Pagan Test:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Breusch-Pagan Test, also known as the White Test, is a statistical test used to detect heteroscedasticity in regression models.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The Breusch-Pagan Test doesn't have a simple formula like other metrics. It involves regressing the squared residuals from the original regression model on the independent variables to test for heteroscedasticity.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Breusch-Pagan Test is used to check for heteroscedasticity, which is a violation of the assumption of constant variance in a regression model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps identify and address heteroscedasticity, ensuring the validity of regression results and parameter estimates.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution and depends on the number of independent variables.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is typically based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, it suggests the presence of heteroscedasticity.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) indicates the presence of heteroscedasticity, meaning that the variances of the residuals are not constant.\n",
    "\n",
    "8. **When to use:**\n",
    "   - The Breusch-Pagan Test is suitable for checking for heteroscedasticity in regression models, especially in cases where the assumption of constant variance is crucial.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other regression model assumptions or when heteroscedasticity is not a concern.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing other regression assumptions, use metrics like the Durbin-Watson Statistic or normality tests. To address heteroscedasticity, consider transforming variables or using robust regression techniques.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can perform the Breusch-Pagan Test in Python using the `statsmodels` library. Here's an example of how to conduct the test:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    bp_test = sm.stats.diagnostic.het_breuschpagan(model.resid, model.model.exog)\n",
    "    ```\n",
    "\n",
    "This code performs the Breusch-Pagan Test on the residuals of a regression model using the `het_breuschpagan` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d9c79",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**White's Test:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - White's Test, also known as the White Heteroscedasticity Test, is a statistical test used to detect heteroscedasticity in regression models.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - White's Test involves regressing the squared residuals from the original regression model on the independent variables, allowing for different weights or corrections to test for heteroscedasticity.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - White's Test is used to check for heteroscedasticity, which is a violation of the assumption of constant variance in a regression model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps identify and address heteroscedasticity, ensuring the validity of regression results and parameter estimates.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution and depends on the number of independent variables and the number of lags used in the test.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is typically based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, it suggests the presence of heteroscedasticity.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) indicates the presence of heteroscedasticity, meaning that the variances of the residuals are not constant.\n",
    "\n",
    "8. **When to use:**\n",
    "   - White's Test is suitable for checking for heteroscedasticity in regression models, especially when the assumption of constant variance is crucial.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other regression model assumptions or when heteroscedasticity is not a concern.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing other regression assumptions, use metrics like the Durbin-Watson Statistic or normality tests. To address heteroscedasticity, consider transforming variables or using robust regression techniques.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can perform White's Test in Python using the `statsmodels` library. Here's an example of how to conduct the test:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    white_test = sm.stats.diagnostic.het_white(model.resid, model.model.exog)\n",
    "    ```\n",
    "\n",
    "This code performs White's Test on the residuals of a regression model using the `het_white` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9a03d",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**Jarque-Bera Test:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Jarque-Bera Test is a statistical test used to assess the normality of the residuals in a regression model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The test statistic is calculated as: \\[JB = \\frac{n}{6} \\left(S^2 + \\frac{1}{4}(K-3)^2\\right)\\]\n",
    "     Where:\n",
    "     - \\(JB\\) is the Jarque-Bera test statistic.\n",
    "     - \\(n\\) is the number of observations.\n",
    "     - \\(S\\) is the sample skewness of the residuals.\n",
    "     - \\(K\\) is the sample kurtosis of the residuals.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Jarque-Bera Test is used to determine whether the residuals from a regression model follow a normal distribution, which is an assumption of linear regression.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps assess the normality of residuals, ensuring that the underlying assumptions of a linear regression model are met.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution with two degrees of freedom under the null hypothesis of normality.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, it suggests that the residuals are not normally distributed.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) indicates that the residuals do not follow a normal distribution.\n",
    "\n",
    "8. **When to use:**\n",
    "   - The Jarque-Bera Test is suitable for checking the normality of residuals in regression models.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other regression model assumptions or for non-parametric regression models.\n",
    "\n",
    "10. **Better alternative:**\n",
    "     - For assessing other regression assumptions, use metrics specific to those assumptions. To address non-normality, consider data transformations or robust regression techniques.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can perform the Jarque-Bera Test in Python using the `statsmodels` library. Here's an example of how to conduct the test:\n",
    "\n",
    "    ```python\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Assuming model is your regression model\n",
    "    jb_test = sm.stats.jarque_bera(model.resid)\n",
    "    ```\n",
    "\n",
    "This code performs the Jarque-Bera Test on the residuals of a regression model using the `jarque_bera` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3bc4fc",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**Ljung-Box Test:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Ljung-Box Test is a statistical test used to check for autocorrelation in time series data or the residuals of a time series model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The Ljung-Box Test statistic is calculated as: \\[Q = n(n+2) \\sum_{k=1}^{h} \\frac{r_k^2}{n-k}\\]\n",
    "     Where:\n",
    "     - \\(Q\\) is the Ljung-Box test statistic.\n",
    "     - \\(n\\) is the number of observations.\n",
    "     - \\(h\\) is the number of lags being tested.\n",
    "     - \\(r_k\\) are the autocorrelations of the series at lag \\(k\\).\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Ljung-Box Test is used to determine whether there is significant autocorrelation in time series data or the residuals of a time series model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps in identifying and addressing autocorrelation, which is essential for the validity of time series models.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution with \\(h\\) degrees of freedom.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, it suggests the presence of autocorrelation.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) indicates the presence of autocorrelation in the data or residuals.\n",
    "\n",
    "8. **When to use:**\n",
    "   - The Ljung-Box Test is suitable for checking for autocorrelation in time series data or the residuals of time series models.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other regression model assumptions unrelated to time series data.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing other regression assumptions, use metrics specific to those assumptions. To address autocorrelation, consider differencing or autoregressive models.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can perform the Ljung-Box Test in Python using libraries like `statsmodels`. Here's an example of how to conduct the test:\n",
    "\n",
    "    ```python\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "    # Assuming residuals is your time series residuals and lags is the number of lags to test\n",
    "    lb_test_stat, lb_p_value = acorr_ljungbox(residuals, lags=[1, 2, 3])\n",
    "    ```\n",
    "\n",
    "This code performs the Ljung-Box Test on time series residuals using the `acorr_ljungbox` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6da4af",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**Portmanteau Test:**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Portmanteau Test is a statistical test used to check for autocorrelation in time series data or the residuals of a time series model.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - The Portmanteau Test doesn't have a simple formula, but it assesses autocorrelation in a time series by testing whether a group of autocorrelations are collectively significantly different from zero.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - The Portmanteau Test is used to determine whether there is significant autocorrelation in time series data or the residuals of a time series model.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - It helps in identifying and addressing autocorrelation, which is essential for the validity of time series models.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The test statistic follows a chi-squared distribution with degrees of freedom determined by the number of lags tested.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold is based on a chosen significance level (alpha). If the p-value associated with the test is less than alpha, it suggests the presence of autocorrelation.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - A small p-value (typically less than alpha) indicates the presence of autocorrelation in the data or residuals.\n",
    "\n",
    "8. **When to use:**\n",
    "   - The Portmanteau Test is suitable for checking for autocorrelation in time series data or the residuals of time series models.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing other regression model assumptions unrelated to time series data.\n",
    "\n",
    "10. **Better alternative:**\n",
    "     - For assessing other regression assumptions, use metrics specific to those assumptions. To address autocorrelation, consider differencing or autoregressive models.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - You can perform the Portmanteau Test in Python using libraries like `statsmodels`. Here's an example of how to conduct the test:\n",
    "\n",
    "    ```python\n",
    "    from statsmodels.stats.diagnostic import acorr_breusch_godfrey\n",
    "\n",
    "    # Assuming residuals is your time series residuals and lags is the number of lags to test\n",
    "    pg_test_stat, pg_p_value = acorr_breusch_godfrey(residuals, nlags=3)\n",
    "    ```\n",
    "\n",
    "This code performs the Portmanteau Test on time series residuals using the `acorr_breusch_godfrey` function from the `statsmodels` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfd73c",
   "metadata": {},
   "source": [
    "Let's continue with the information for the next residual analysis metric:\n",
    "\n",
    "**Cumulative Sum of Residuals (CUSUM):**\n",
    "\n",
    "1. **What is the metric?**\n",
    "   - The Cumulative Sum of Residuals (CUSUM) is a statistical technique used to detect structural changes or shifts in a time series, making it valuable for quality control and monitoring.\n",
    "\n",
    "2. **Formula and explanation:**\n",
    "   - CUSUM involves calculating the cumulative sum of the residuals over time. By monitoring these cumulative sums, one can detect deviations or changes in the underlying process.\n",
    "\n",
    "3. **Why is it used?**\n",
    "   - CUSUM is used to identify shifts or changes in a time series, which can indicate problems or variations in a process.\n",
    "\n",
    "4. **Benefit of using it:**\n",
    "   - CUSUM helps in early detection of structural changes, making it valuable for quality control and process monitoring.\n",
    "\n",
    "5. **Range of the output:**\n",
    "   - The CUSUM values can vary depending on the time series and the presence of structural changes.\n",
    "\n",
    "6. **Ideal threshold:**\n",
    "   - The ideal threshold depends on the specific application and the tolerance for false alarms. A threshold is set to signal a change when the CUSUM values exceed it.\n",
    "\n",
    "7. **How to interpret the results:**\n",
    "   - When CUSUM values exceed the threshold, it indicates a potential shift or change in the underlying process.\n",
    "\n",
    "8. **When to use:**\n",
    "   - CUSUM is suitable for continuous monitoring and early detection of changes in processes or time series data.\n",
    "\n",
    "9. **When not to use:**\n",
    "   - It is not typically used for assessing regression assumptions but is valuable for monitoring and quality control.\n",
    "\n",
    "10. **Better alternative:**\n",
    "    - For assessing regression assumptions or model performance, use metrics specific to those purposes. To monitor processes and detect structural changes, CUSUM is a valuable technique.\n",
    "\n",
    "11. **Code snippet (Python):**\n",
    "    - Calculating CUSUM typically involves custom implementation based on the specific application. Here's a simplified example of how to calculate CUSUM for a time series using Python:\n",
    "\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming data is your time series data\n",
    "    cusum = np.cumsum(data - np.mean(data))\n",
    "    ```\n",
    "\n",
    "This code calculates the CUSUM values by taking the cumulative sum of deviations from the mean of the time series. The specific implementation may vary based on the application and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bce064",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b3b955",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c59f3fda",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882eed68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b06b0ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acfbb604",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d81feb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eeacc56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bccedc47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78a579b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "639c12f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f16312",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d90a56dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a378ebc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0699ac2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "424e4bb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3ea249e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8617174",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601aa400",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31be1963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a16151d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51b46c5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01bac0ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1b86b14",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fce3d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d6931b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccbe0d0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a4141d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1af3f32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13066512",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a5166a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c5235e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d5ae490",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b70c1b3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6182ecb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c1da55a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d6126e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dbbe6b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d770a6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97853c9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8dcbbb0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e96e8a93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4251377d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18413f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee0390b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959ebe62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13665b3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7ab5974",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6049648e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01e119b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c067afa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
